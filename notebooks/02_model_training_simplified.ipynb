{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînement du modèle de classification des prunes africaines\n",
    "\n",
    "Ce notebook utilise les fonctions existantes dans le dépôt pour entraîner le modèle de classification des prunes africaines en utilisant le jeu de données Kaggle \"African Plums Dataset\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration de l'environnement pour Google Colab\n",
    "\n",
    "Commençons par cloner le dépôt GitHub et configurer l'environnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier si nous sommes dans Google Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"Exécution dans Google Colab: {IN_COLAB}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Cloner le dépôt GitHub\n",
    "    !git clone https://github.com/CodeStorm-mbe/african-plums-classifier.git\n",
    "    %cd african-plums-classifier\n",
    "    \n",
    "    # Installer les dépendances requises\n",
    "    !pip install -r requirements.txt\n",
    "    !pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Monter Google Drive pour la persistance des données\n",
    "\n",
    "Pour conserver les données et les modèles entre les différents notebooks, nous allons utiliser Google Drive comme stockage persistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monter Google Drive si nous sommes dans Colab\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Définir les chemins dans Google Drive\n",
    "    DRIVE_PROJECT_DIR = \"/content/drive/MyDrive/african-plums-classifier\"\n",
    "    DRIVE_DATA_DIR = f\"{DRIVE_PROJECT_DIR}/data\"\n",
    "    DRIVE_KAGGLE_DIR = f\"{DRIVE_DATA_DIR}/kaggle\"\n",
    "    DRIVE_RAW_DATA_DIR = f\"{DRIVE_DATA_DIR}/raw\"\n",
    "    DRIVE_PLUM_DATA_DIR = f\"{DRIVE_RAW_DATA_DIR}/plums\"\n",
    "    DRIVE_NON_PLUM_DATA_DIR = f\"{DRIVE_RAW_DATA_DIR}/non_plums\"\n",
    "    DRIVE_MODELS_DIR = f\"{DRIVE_PROJECT_DIR}/models\"\n",
    "    \n",
    "    # Vérifier si les répertoires existent, sinon les créer\n",
    "    !mkdir -p {DRIVE_PROJECT_DIR}\n",
    "    !mkdir -p {DRIVE_DATA_DIR}\n",
    "    !mkdir -p {DRIVE_KAGGLE_DIR}\n",
    "    !mkdir -p {DRIVE_RAW_DATA_DIR}\n",
    "    !mkdir -p {DRIVE_PLUM_DATA_DIR}\n",
    "    !mkdir -p {DRIVE_NON_PLUM_DATA_DIR}\n",
    "    !mkdir -p {DRIVE_MODELS_DIR}\n",
    "    \n",
    "    print(f\"Google Drive monté et répertoires créés dans {DRIVE_PROJECT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "# Ajouter le répertoire courant au chemin pour pouvoir importer nos modules\n",
    "if IN_COLAB:\n",
    "    # Dans Colab, nous sommes déjà dans le répertoire du projet\n",
    "    if \"/content/african-plums-classifier\" not in sys.path:\n",
    "        sys.path.append(\"/content/african-plums-classifier\")\n",
    "else:\n",
    "    # En local, ajouter le répertoire parent\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "# Importer nos modules personnalisés\n",
    "from data.data_preprocessing import load_and_prepare_two_stage_data\n",
    "from models.model_architecture import get_model, TwoStageModel\n",
    "from scripts.train_two_stage import train_model, evaluate_model, plot_training_history\n",
    "\n",
    "# Définir les chemins des données\n",
    "if IN_COLAB:\n",
    "    # Utiliser les chemins dans Google Drive pour la persistance\n",
    "    DATA_ROOT = DRIVE_RAW_DATA_DIR\n",
    "    KAGGLE_DIR = DRIVE_KAGGLE_DIR\n",
    "    MODELS_DIR = DRIVE_MODELS_DIR\n",
    "    \n",
    "    # Créer également des liens symboliques pour faciliter l'accès depuis le code existant\n",
    "    LOCAL_DATA_ROOT = \"data/raw\"\n",
    "    LOCAL_KAGGLE_DIR = \"data/kaggle\"\n",
    "    LOCAL_MODELS_DIR = \"models/saved\"\n",
    "    \n",
    "    # Créer les répertoires locaux s'ils n'existent pas\n",
    "    !mkdir -p {LOCAL_DATA_ROOT}\n",
    "    !mkdir -p {LOCAL_KAGGLE_DIR}\n",
    "    !mkdir -p {LOCAL_MODELS_DIR}\n",
    "    \n",
    "    # Créer des liens symboliques si nécessaire\n",
    "    if not os.path.exists(LOCAL_DATA_ROOT) or not os.path.islink(LOCAL_DATA_ROOT):\n",
    "        !rm -rf {LOCAL_DATA_ROOT}\n",
    "        !ln -s {DATA_ROOT} {LOCAL_DATA_ROOT}\n",
    "    \n",
    "    if not os.path.exists(LOCAL_KAGGLE_DIR) or not os.path.islink(LOCAL_KAGGLE_DIR):\n",
    "        !rm -rf {LOCAL_KAGGLE_DIR}\n",
    "        !ln -s {KAGGLE_DIR} {LOCAL_KAGGLE_DIR}\n",
    "        \n",
    "    if not os.path.exists(LOCAL_MODELS_DIR) or not os.path.islink(LOCAL_MODELS_DIR):\n",
    "        !rm -rf {LOCAL_MODELS_DIR}\n",
    "        !ln -s {MODELS_DIR} {LOCAL_MODELS_DIR}\n",
    "else:\n",
    "    # En local\n",
    "    DATA_ROOT = \"../data/raw\"\n",
    "    KAGGLE_DIR = \"../data/kaggle\"\n",
    "    MODELS_DIR = \"../models/saved\"\n",
    "\n",
    "PLUM_DATA_DIR = os.path.join(DATA_ROOT, \"plums\")  # Sous-dossier pour les prunes\n",
    "NON_PLUM_DATA_DIR = os.path.join(DATA_ROOT, \"non_plums\")  # Sous-dossier pour les non-prunes\n",
    "\n",
    "# Créer les répertoires s'ils n'existent pas\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(PLUM_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(NON_PLUM_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(KAGGLE_DIR, exist_ok=True)\n",
    "\n",
    "# Définir les paramètres d'entraînement\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 224\n",
    "NUM_WORKERS = 2 if IN_COLAB else 4  # Réduire le nombre de workers dans Colab\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10 if IN_COLAB else 25  # Réduire le nombre d'époques dans Colab pour accélérer\n",
    "EARLY_STOPPING_PATIENCE = 3 if IN_COLAB else 7  # Réduire la patience dans Colab\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Fixer les seeds pour la reproductibilité\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Déterminer le device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilisation de: {device}\")\n",
    "\n",
    "# Afficher les chemins des données\n",
    "print(f\"\\nChemins des données:\")\n",
    "print(f\"DATA_ROOT: {DATA_ROOT}\")\n",
    "print(f\"KAGGLE_DIR: {KAGGLE_DIR}\")\n",
    "print(f\"PLUM_DATA_DIR: {PLUM_DATA_DIR}\")\n",
    "print(f\"NON_PLUM_DATA_DIR: {NON_PLUM_DATA_DIR}\")\n",
    "print(f\"MODELS_DIR: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vérification des données préparées dans Google Drive\n",
    "\n",
    "Vérifions si les données ont déjà été préparées dans le notebook précédent et sont disponibles dans Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier si les données ont été préparées dans le notebook précédent\n",
    "def check_data_preparation():\n",
    "    \"\"\"Vérifie si les données ont été préparées dans le notebook précédent.\"\"\"\n",
    "    if IN_COLAB:\n",
    "        # Vérifier si le fichier d'informations existe dans Google Drive\n",
    "        data_prep_info_path = f\"{DRIVE_PROJECT_DIR}/data_prep_info.json\"\n",
    "        if os.path.exists(data_prep_info_path):\n",
    "            try:\n",
    "                with open(data_prep_info_path, 'r') as f:\n",
    "                    data_prep_info = json.load(f)\n",
    "                \n",
    "                if data_prep_info.get('data_prepared', False):\n",
    "                    print(f\"Les données ont été préparées le {data_prep_info.get('date_prepared', 'date inconnue')}.\")\n",
    "                    return data_prep_info\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors de la lecture du fichier d'informations: {e}\")\n",
    "    \n",
    "    # Vérifier si les répertoires de données existent et contiennent des images\n",
    "    plum_classes = [d for d in os.listdir(PLUM_DATA_DIR) if os.path.isdir(os.path.join(PLUM_DATA_DIR, d))]\n",
    "    if not plum_classes:\n",
    "        print(f\"Aucune classe de prune trouvée dans {PLUM_DATA_DIR}.\")\n",
    "        return None\n",
    "    \n",
    "    non_plum_dir = os.path.join(NON_PLUM_DATA_DIR, \"non_plum\")\n",
    "    if not os.path.exists(non_plum_dir):\n",
    "        print(f\"Le répertoire {non_plum_dir} n'existe pas.\")\n",
    "        return None\n",
    "    \n",
    "    # Vérifier si les répertoires contiennent des images\n",
    "    has_plum_images = False\n",
    "    for cls in plum_classes:\n",
    "        cls_dir = os.path.join(PLUM_DATA_DIR, cls)\n",
    "        images = [f for f in os.listdir(cls_dir) if os.path.isfile(os.path.join(cls_dir, f))]\n",
    "        if images:\n",
    "            has_plum_images = True\n",
    "            break\n",
    "    \n",
    "    has_non_plum_images = False\n",
    "    images = [f for f in os.listdir(non_plum_dir) if os.path.isfile(os.path.join(non_plum_dir, f))]\n",
    "    if images:\n",
    "        has_non_plum_images = True\n",
    "    \n",
    "    if has_plum_images and has_non_plum_images:\n",
    "        print(\"Les données semblent être préparées (images trouvées dans les répertoires).\")\n",
    "        return {\n",
    "            'data_prepared': True,\n",
    "            'plum_classes': plum_classes,\n",
    "            'detection_class_names': ['plum', 'non_plum'],\n",
    "            'classification_class_names': plum_classes\n",
    "        }\n",
    "    else:\n",
    "        print(\"Les données ne semblent pas être complètement préparées.\")\n",
    "        return None\n",
    "\n",
    "# Vérifier si les données ont été préparées\n",
    "data_prep_info = check_data_preparation()\n",
    "\n",
    "if data_prep_info:\n",
    "    print(\"\\nInformations sur les données préparées:\")\n",
    "    for key, value in data_prep_info.items():\n",
    "        print(f\"  - {key}: {value}\")\n",
    "else:\n",
    "    print(\"\\nVeuillez d'abord exécuter le notebook de préparation des données.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chargement des données pour l'entraînement\n",
    "\n",
    "Chargeons les données préparées pour l'entraînement du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données pour l'entraînement\n",
    "def load_data_for_training():\n",
    "    \"\"\"Charge les données pour l'entraînement du modèle.\"\"\"\n",
    "    try:\n",
    "        # Charger et préparer les données pour les deux étapes\n",
    "        print(\"Chargement des données pour les deux étapes...\")\n",
    "        (detection_train_loader, detection_val_loader, detection_test_loader, detection_class_names), \\\n",
    "        (classification_train_loader, classification_val_loader, classification_test_loader, classification_class_names) = \\\n",
    "            load_and_prepare_two_stage_data(\n",
    "                PLUM_DATA_DIR, \n",
    "                NON_PLUM_DATA_DIR,\n",
    "                batch_size=BATCH_SIZE, \n",
    "                img_size=IMG_SIZE,\n",
    "                num_workers=NUM_WORKERS\n",
    "            )\n",
    "        \n",
    "        print(f\"Classes de détection: {detection_class_names}\")\n",
    "        print(f\"Classes de classification: {classification_class_names}\")\n",
    "        \n",
    "        # Afficher les tailles des datasets\n",
    "        print(f\"\\nTailles des datasets de détection:\")\n",
    "        print(f\"  - Entraînement: {len(detection_train_loader.dataset)} images\")\n",
    "        print(f\"  - Validation: {len(detection_val_loader.dataset)} images\")\n",
    "        print(f\"  - Test: {len(detection_test_loader.dataset)} images\")\n",
    "        \n",
    "        print(f\"\\nTailles des datasets de classification:\")\n",
    "        print(f\"  - Entraînement: {len(classification_train_loader.dataset)} images\")\n",
    "        print(f\"  - Validation: {len(classification_val_loader.dataset)} images\")\n",
    "        print(f\"  - Test: {len(classification_test_loader.dataset)} images\")\n",
    "        \n",
    "        return {\n",
    "            'detection': {\n",
    "                'train_loader': detection_train_loader,\n",
    "                'val_loader': detection_val_loader,\n",
    "                'test_loader': detection_test_loader,\n",
    "                'class_names': detection_class_names\n",
    "            },\n",
    "            'classification': {\n",
    "                'train_loader': classification_train_loader,\n",
    "                'val_loader': classification_val_loader,\n",
    "                'test_loader': classification_test_loader,\n",
    "                'class_names': classification_class_names\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement des données: {e}\")\n",
    "        return None\n",
    "\n",
    "# Charger les données si elles ont été préparées\n",
    "if data_prep_info and data_prep_info.get('data_prepared', False):\n",
    "    data_loaders = load_data_for_training()\n",
    "    \n",
    "    if data_loaders:\n",
    "        detection_class_names = data_loaders['detection']['class_names']\n",
    "        classification_class_names = data_loaders['classification']['class_names']\n",
    "        \n",
    "        detection_train_loader = data_loaders['detection']['train_loader']\n",
    "        detection_val_loader = data_loaders['detection']['val_loader']\n",
    "        detection_test_loader = data_loaders['detection']['test_loader']\n",
    "        \n",
    "        classification_train_loader = data_loaders['classification']['train_loader']\n",
    "        classification_val_loader = data_loaders['classification']['val_loader']\n",
    "        classification_test_loader = data_loaders['classification']['test_loader']\n",
    "        \n",
    "        print(\"\\nDonnées chargées avec succès pour l'entraînement.\")\n",
    "    else:\n",
    "        print(\"\\nErreur lors du chargement des données. Veuillez vérifier les données préparées.\")\n",
    "else:\n",
    "    print(\"\\nVeuillez d'abord exécuter le notebook de préparation des données.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Définition des modèles\n",
    "\n",
    "Définissons les modèles pour la détection et la classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les modèles\n",
    "def create_models():\n",
    "    \"\"\"Crée les modèles pour la détection et la classification.\"\"\"\n",
    "    if 'detection_class_names' not in locals() or 'classification_class_names' not in locals():\n",
    "        print(\"Les noms des classes ne sont pas définis. Veuillez d'abord charger les données.\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # Modèle de détection (prune vs non-prune)\n",
    "        print(\"Création du modèle de détection...\")\n",
    "        detection_model = get_model(\n",
    "            model_name=\"standard\",\n",
    "            num_classes=len(detection_class_names),\n",
    "            base_model=\"resnet18\",\n",
    "            pretrained=True\n",
    "        )\n",
    "        detection_model = detection_model.to(device)\n",
    "        \n",
    "        # Modèle de classification (types de prunes)\n",
    "        print(\"Création du modèle de classification...\")\n",
    "        classification_model = get_model(\n",
    "            model_name=\"standard\",\n",
    "            num_classes=len(classification_class_names),\n",
    "            base_model=\"resnet34\",\n",
    "            pretrained=True\n",
    "        )\n",
    "        classification_model = classification_model.to(device)\n",
    "        \n",
    "        return detection_model, classification_model\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la création des modèles: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Créer les modèles si les données ont été chargées\n",
    "if 'detection_class_names' in locals() and 'classification_class_names' in locals():\n",
    "    detection_model, classification_model = create_models()\n",
    "    \n",
    "    if detection_model is not None and classification_model is not None:\n",
    "        print(\"\\nModèles créés avec succès.\")\n",
    "        print(f\"Modèle de détection: {detection_model.__class__.__name__}\")\n",
    "        print(f\"Modèle de classification: {classification_model.__class__.__name__}\")\n",
    "    else:\n",
    "        print(\"\\nErreur lors de la création des modèles.\")\n",
    "else:\n",
    "    print(\"\\nVeuillez d'abord charger les données.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entraînement du modèle de détection\n",
    "\n",
    "Entraînons d'abord le modèle de détection pour distinguer les prunes des non-prunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le modèle de détection\n",
    "def train_detection_model():\n",
    "    \"\"\"Entraîne le modèle de détection.\"\"\"\n",
    "    if 'detection_model' not in locals() or detection_model is None:\n",
    "        print(\"Le modèle de détection n'est pas défini. Veuillez d'abord créer les modèles.\")\n",
    "        return None\n",
    "    \n",
    "    if 'detection_train_loader' not in locals() or 'detection_val_loader' not in locals():\n",
    "        print(\"Les données d'entraînement ne sont pas définies. Veuillez d'abord charger les données.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Définir l'optimiseur et le scheduler\n",
    "        optimizer = optim.Adam(detection_model.parameters(), lr=LEARNING_RATE)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Entraîner le modèle\n",
    "        print(\"Entraînement du modèle de détection...\")\n",
    "        detection_history = train_model(\n",
    "            model=detection_model,\n",
    "            train_loader=detection_train_loader,\n",
    "            val_loader=detection_val_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            device=device,\n",
    "            early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "            model_save_path=os.path.join(MODELS_DIR, 'detection_best_acc.pth')\n",
    "        )\n",
    "        \n",
    "        # Tracer l'historique d'entraînement\n",
    "        plot_training_history(detection_history)\n",
    "        \n",
    "        # Sauvegarder le graphique dans Google Drive si nous sommes dans Colab\n",
    "        if IN_COLAB and os.path.exists('training_history.png'):\n",
    "            detection_history_img_path = f\"{DRIVE_PROJECT_DIR}/detection_training_history.png\"\n",
    "            shutil.copy('training_history.png', detection_history_img_path)\n",
    "            print(f\"Graphique d'historique d'entraînement sauvegardé dans Google Drive: {detection_history_img_path}\")\n",
    "        \n",
    "        return detection_history\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'entraînement du modèle de détection: {e}\")\n",
    "        return None\n",
    "\n",
    "# Entraîner le modèle de détection si les modèles ont été créés\n",
    "if 'detection_model' in locals() and detection_model is not None and 'detection_train_loader' in locals():\n",
    "    detection_history = train_detection_model()\n",
    "    \n",
    "    if detection_history is not None:\n",
    "        print(\"\\nModèle de détection entraîné avec succès.\")\n",
    "    else:\n",
    "        print(\"\\nErreur lors de l'entraînement du modèle de détection.\")\n",
    "else:\n",
    "    print(\"\\nVeuillez d'abord créer les modèles et charger les données.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Évaluation du modèle de détection\n",
    "\n",
    "Évaluons les performances du modèle de détection sur l'ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer le modèle de détection\n",
    "def evaluate_detection_model():\n",
    "    \"\"\"Évalue le modèle de détection sur l'ensemble de test.\"\"\"\n",
    "    if 'detection_model' not in locals() or detection_model is None:\n",
    "        print(\"Le modèle de détection n'est pas défini. Veuillez d'abord créer les modèles.\")\n",
    "        return None\n",
    "    \n",
    "    if 'detection_test_loader' not in locals():\n",
    "        print(\"Les données de test ne sont pas définies. Veuillez d'abord charger les données.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Charger le meilleur modèle\n",
    "        best_model_path = os.path.join(MODELS_DIR, 'detection_best_acc.pth')\n",
    "        if os.path.exists(best_model_path):\n",
    "            detection_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "            print(f\"Meilleur modèle de détection chargé depuis {best_model_path}\")\n",
    "        \n",
    "        # Évaluer le modèle\n",
    "        print(\"Évaluation du modèle de détection sur l'ensemble de test...\")\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        test_loss, test_acc, test_f1, confusion_mat = evaluate_model(\n",
    "            model=detection_model,\n",
    "            test_loader=detection_test_loader,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            class_names=detection_class_names\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nRésultats sur l'ensemble de test:\")\n",
    "        print(f\"  - Perte: {test_loss:.4f}\")\n",
    "        print(f\"  - Précision: {test_acc:.4f}\")\n",
    "        print(f\"  - Score F1: {test_f1:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'loss': test_loss,\n",
    "            'accuracy': test_acc,\n",
    "            'f1_score': test_f1,\n",
    "            'confusion_matrix': confusion_mat\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'évaluation du modèle de détection: {e}\")\n",
    "        return None\n",
    "\n",
    "# Évaluer le modèle de détection si le modèle a été entraîné\n",
    "if 'detection_model' in locals() and detection_model is not None and 'detection_test_loader' in locals():\n",
    "    detection_results = evaluate_detection_model()\n",
    "    \n",
    "    if detection_results is not None:\n",
    "        print(\"\\nModèle de détection évalué avec succès.\")\n",
    "    else:\n",
    "        print(\"\\nErreur lors de l'évaluation du modèle de détection.\")\n",
    "else:\n",
    "    print(\"\\nVeuillez d'abord entraîner le modèle de détection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Entraînement du modèle de classification\n",
    "\n",
    "Entraînons maintenant le modèle de classification pour distinguer les différents types de prunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le modèle de classification\n",
    "def train_classification_model():\n",
    "    \"\"\"Entraîne le modèle de classification.\"\"\"\n",
    "    if 'classification_model' not in locals() or classification_model is None:\n",
    "        print(\"Le modèle de classification n'est pas défini. Veuillez d'abord créer les modèles.\")\n",
    "        return None\n",
    "    \n",
    "    if 'classification_train_loader' not in locals() or 'classification_val_loader' not in locals():\n",
    "        print(\"Les données d'entraînement ne sont pas définies. Veuillez d'abord charger les données.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Définir l'optimiseur et le scheduler\n",
    "        optimizer = optim.Adam(classification_model.parameters(), lr=LEARNING_RATE)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Entraîner le modèle\n",
    "        print(\"Entraînement du modèle de classification...\")\n",
    "        classification_history = train_model(\n",
    "            model=classification_model,\n",
    "            train_loader=classification_train_loader,\n",
    "            val_loader=classification_val_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            device=device,\n",
    "            early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "            model_save_path=os.path.join(MODELS_DIR, 'classification_best_acc.pth')\n",
    "        )\n",
    "        \n",
    "        # Tracer l'historique d'entraînement\n",
    "        plot_training_history(classification_history)\n",
    "        \n",
    "        # Sauvegarder le graphique dans Google Drive si nous sommes dans Colab\n",
    "        if IN_COLAB and os.path.exists('training_history.png'):\n",
    "            classification_history_img_path = f\"{DRIVE_PROJECT_DIR}/classification_training_history.png\"\n",
    "            shutil.copy('training_history.png', classification_history_img_path)\n",
    "            print(f\"Graphique d'historique d'entraînement sauvegardé dans Google Drive: {classification_history_img_path}\")\n",
    "        \n",
    "        return classification_history\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'entraînement du modèle de classification: {e}\")\n",
    "        return None\n",
    "\n",
    "# Entraîner le modèle de classification si les modèles ont été créés\n",
    "if 'classification_model' in locals() and classification_model is not None and 'classification_train_loader' in locals():\n",
    "    classification_history = train_classification_model()\n",
    "    \n",
    "    if classification_history is not None:\n",
    "        print(\"\\nModèle de classification entraîné avec succès.\")\n",
    "    else:\n",
    "        print(\"\\nErreur lors de l'entraînement du modèle de classification.\")\n",
    "else:\n",
    "    print(\"\\nVeuillez d'abord créer les modèles et charger les données.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Évaluation du modèle de classification\n",
    "\n",
    "Évaluons les performances du modèle de classification sur l'ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer le modèle de classification\n",
    "def evaluate_classification_model():\n",
    "    \"\"\"Évalue le modèle de classification sur l'ensemble de test.\"\"\"\n",
    "    if 'classification_model' not in locals() or classification_model is None:\n",
    "        print(\"Le modèle de classification n'est pas défini. Veuillez d'abord créer les modèles.\")\n",
    "        return None\n",
    "    \n",
    "    if 'classification_test_loader' not in locals():\n",
    "        print(\"Les données de test ne sont pas définies. Veuillez d'abord charger les données.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Charger le meilleur modèle\n",
    "        best_model_path = os.path.join(MODELS_DIR, 'classification_best_acc.pth')\n",
    "        if os.path.exists(best_model_path):\n",
    "            classification_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "            print(f\"Meilleur modèle de classification chargé depuis {best_model_path}\")\n",
    "        \n",
    "        # Évaluer le modèle\n",
    "        print(\"Évaluation du modèle de classification sur l'ensemble de test...\")\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        test_loss, test_acc, test_f1, confusion_mat = evaluate_model(\n",
    "            model=classification_model,\n",
    "            test_loader=classification_test_loader,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            class_names=classification_class_names\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nRésultats sur l'ensemble de test:\")\n",
    "        print(f\"  - Perte: {test_loss:.4f}\")\n",
    "        print(f\"  - Précision: {test_acc:.4f}\")\n",
    "        print(f\"  - Score F1: {test_f1:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'loss': test_loss,\n",
    "            'accuracy': test_acc,\n",
    "            'f1_score': test_f1,\n",
    "            'confusion_matrix': confusion_mat\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'évaluation du modèle de classification: {e}\")\n",
    "        return None\n",
    "\n",
    "# Évaluer le modèle de classification si le modèle a été entraîné\n",
    "if 'classification_model' in locals() and classification_model is not None and 'classification_test_loader' in locals():\n",
    "    classification_results = evaluate_classification_model()\n",
    "    \n",
    "    if classification_results is not None:\n",
    "        print(\"\\nModèle de classification évalué avec succès.\")\n",
    "    else:\n",
    "        print(\"\\nErreur lors de l'évaluation du modèle de classification.\")\n",
    "else:\n",
    "    print(\"\\nVeuillez d'abord entraîner le modèle de classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Création et sauvegarde du modèle à deux étapes\n",
    "\n",
    "Créons et sauvegardons le modèle à deux étapes qui combine le modèle de détection et le modèle de classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer et sauvegarder le modèle à deux étapes\n",
    "def create_and_save_two_stage_model():\n",
    "    \"\"\"Crée et sauvegarde le modèle à deux étapes.\"\"\"\n",
    "    if 'detection_model' not in locals() or detection_model is None or 'classification_model' not in locals() or classification_model is None:\n",
    "        print(\"Les modèles de détection et de classification ne sont pas définis. Veuillez d'abord créer les modèles.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Charger les meilleurs modèles\n",
    "        detection_best_path = os.path.join(MODELS_DIR, 'detection_best_acc.pth')\n",
    "        classification_best_path = os.path.join(MODELS_DIR, 'classification_best_acc.pth')\n",
    "        \n",
    "        if os.path.exists(detection_best_path) and os.path.exists(classification_best_path):\n",
    "            detection_model.load_state_dict(torch.load(detection_best_path, map_location=device))\n",
    "            classification_model.load_state_dict(torch.load(classification_best_path, map_location=device))\n",
    "            print(\"Meilleurs modèles chargés avec succès.\")\n",
    "        else:\n",
    "            print(\"Les fichiers des meilleurs modèles n'existent pas. Utilisation des modèles actuels.\")\n",
    "        \n",
    "        # Créer le modèle à deux étapes\n",
    "        print(\"Création du modèle à deux étapes...\")\n",
    "        two_stage_model = TwoStageModel(\n",
    "            detection_model=detection_model,\n",
    "            classification_model=classification_model,\n",
    "            detection_threshold=0.7  # Seuil de confiance pour la détection\n",
    "        )\n",
    "        \n",
    "        # Sauvegarder les informations du modèle\n",
    "        model_info = {\n",
    "            'detection_classes': detection_class_names,\n",
    "            'classification_classes': classification_class_names,\n",
    "            'model_info': {\n",
    "                'detection_model': {\n",
    "                    'base_model': 'standard_resnet18',\n",
    "                    'num_classes': len(detection_class_names)\n",
    "                },\n",
    "                'classification_model': {\n",
    "                    'base_model': 'standard_resnet34',\n",
    "                    'num_classes': len(classification_class_names)\n",
    "                },\n",
    "                'detection_threshold': 0.7\n",
    "            },\n",
    "            'date_created': time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        # Sauvegarder les informations du modèle\n",
    "        model_info_path = os.path.join(MODELS_DIR, 'two_stage_model_info.json')\n",
    "        with open(model_info_path, 'w') as f:\n",
    "            json.dump(model_info, f, indent=4)\n",
    "        \n",
    "        print(f\"Informations du modèle à deux étapes sauvegardées dans {model_info_path}\")\n",
    "        \n",
    "        # Sauvegarder les informations d'entraînement dans Google Drive si nous sommes dans Colab\n",
    "        if IN_COLAB:\n",
    "            training_info = {\n",
    "                'detection_results': detection_results if 'detection_results' in locals() else None,\n",
    "                'classification_results': classification_results if 'classification_results' in locals() else None,\n",
    "                'model_info': model_info,\n",
    "                'training_parameters': {\n",
    "                    'batch_size': BATCH_SIZE,\n",
    "                    'img_size': IMG_SIZE,\n",
    "                    'learning_rate': LEARNING_RATE,\n",
    "                    'num_epochs': NUM_EPOCHS,\n",
    "                    'early_stopping_patience': EARLY_STOPPING_PATIENCE\n",
    "                },\n",
    "                'date_trained': time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "            \n",
    "            training_info_path = f\"{DRIVE_PROJECT_DIR}/training_info.json\"\n",
    "            with open(training_info_path, 'w') as f:\n",
    "                json.dump(training_info, f, indent=4)\n",
    "            \n",
    "            print(f\"Informations d'entraînement sauvegardées dans Google Drive: {training_info_path}\")\n",
    "        \n",
    "        return two_stage_model\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la création et de la sauvegarde du modèle à deux étapes: {e}\")\n",
    "        return None\n",
    "\n",
    "# Créer et sauvegarder le modèle à deux étapes si les modèles ont été entraînés\n",
    "if 'detection_model' in locals() and detection_model is not None and 'classification_model' in locals() and classification_model is not None:\n",
    "    two_stage_model = create_and_save_two_stage_model()\n",
    "    \n",
    "    if two_stage_model is not None:\n",
    "        print(\"\\nModèle à deux étapes créé et sauvegardé avec succès.\")\n",
    "    else:\n",
    "        print(\"\\nErreur lors de la création et de la sauvegarde du modèle à deux étapes.\")\n",
    "else:\n",
    "    print(\"\\nVeuillez d'abord entraîner les modèles de détection et de classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "Dans ce notebook, nous avons utilisé les fonctions existantes des modules `data_preprocessing`, `model_architecture` et `train_two_stage` pour :\n",
    "1. Charger les données préparées dans le notebook précédent\n",
    "2. Créer les modèles de détection et de classification\n",
    "3. Entraîner et évaluer les modèles\n",
    "4. Créer et sauvegarder le modèle à deux étapes\n",
    "\n",
    "Les modèles entraînés sont sauvegardés dans Google Drive pour une utilisation ultérieure dans le notebook de test."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
