{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînement du modèle de classification des prunes africaines\n",
    "\n",
    "Ce notebook utilise les fonctions existantes dans le dépôt pour entraîner le modèle de classification des prunes africaines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration de l'environnement pour Google Colab\n",
    "\n",
    "Commençons par cloner le dépôt GitHub et configurer l'environnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier si nous sommes dans Google Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"Exécution dans Google Colab: {IN_COLAB}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Cloner le dépôt GitHub\n",
    "    !git clone https://github.com/CodeStorm-mbe/african-plums-classifier.git\n",
    "    %cd african-plums-classifier\n",
    "    \n",
    "    # Installer les dépendances requises\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Ajouter le répertoire courant au chemin pour pouvoir importer nos modules\n",
    "if IN_COLAB:\n",
    "    # Dans Colab, nous sommes déjà dans le répertoire du projet\n",
    "    if \"/content/african-plums-classifier\" not in sys.path:\n",
    "        sys.path.append(\"/content/african-plums-classifier\")\n",
    "else:\n",
    "    # En local, ajouter le répertoire parent\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "# Importer nos modules personnalisés\n",
    "from data.data_preprocessing import load_and_prepare_two_stage_data\n",
    "from models.model_architecture import get_model, TwoStageModel\n",
    "from scripts.train_two_stage import train_model, evaluate_model, plot_training_history\n",
    "\n",
    "# Définir les chemins des données\n",
    "if IN_COLAB:\n",
    "    # Dans Colab, créer les répertoires dans le dossier du projet cloné\n",
    "    DATA_ROOT = \"data/raw\"\n",
    "    MODELS_DIR = \"models/saved\"\n",
    "else:\n",
    "    # En local\n",
    "    DATA_ROOT = \"../data/raw\"\n",
    "    MODELS_DIR = \"../models/saved\"\n",
    "\n",
    "PLUM_DATA_DIR = os.path.join(DATA_ROOT, \"plums\")  # Sous-dossier pour les prunes\n",
    "NON_PLUM_DATA_DIR = os.path.join(DATA_ROOT, \"non_plums\")  # Sous-dossier pour les non-prunes\n",
    "\n",
    "# Créer les répertoires s'ils n'existent pas\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(PLUM_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(NON_PLUM_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Définir les paramètres d'entraînement\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 224\n",
    "NUM_WORKERS = 2 if IN_COLAB else 4  # Réduire le nombre de workers dans Colab\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10 if IN_COLAB else 25  # Réduire le nombre d'époques dans Colab pour accélérer\n",
    "EARLY_STOPPING_PATIENCE = 3 if IN_COLAB else 7  # Réduire la patience dans Colab\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Fixer les seeds pour la reproductibilité\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Déterminer le device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilisation de: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Création de données d'exemple\n",
    "\n",
    "Si vous n'avez pas exécuté le notebook de préparation des données, créons des données d'exemple pour tester ce notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_data(force_create=False):\n",
    "    \"\"\"Crée des données d'exemple pour tester le notebook.\"\"\"\n",
    "    # Vérifier si des données existent déjà\n",
    "    plum_classes = [d for d in os.listdir(PLUM_DATA_DIR) if os.path.isdir(os.path.join(PLUM_DATA_DIR, d))]\n",
    "    non_plum_dir = os.path.join(NON_PLUM_DATA_DIR, \"non_plum\")\n",
    "    \n",
    "    if plum_classes and os.path.exists(non_plum_dir) and not force_create:\n",
    "        print(\"Des données existent déjà. Utilisez force_create=True pour les remplacer.\")\n",
    "        return\n",
    "    \n",
    "    # Créer la structure de dossiers\n",
    "    plum_classes = [\"ripe\", \"unripe\", \"damaged\", \"diseased\", \"overripe\", \"healthy\"]\n",
    "    for cls in plum_classes:\n",
    "        os.makedirs(os.path.join(PLUM_DATA_DIR, cls), exist_ok=True)\n",
    "    \n",
    "    os.makedirs(non_plum_dir, exist_ok=True)\n",
    "    \n",
    "    # Créer des images d'exemple (carrés colorés)\n",
    "    colors = {\n",
    "        \"ripe\": (150, 0, 0),      # Rouge foncé\n",
    "        \"unripe\": (0, 150, 0),    # Vert foncé\n",
    "        \"damaged\": (150, 100, 0), # Marron\n",
    "        \"diseased\": (100, 0, 100),# Violet\n",
    "        \"overripe\": (100, 0, 0),  # Rouge très foncé\n",
    "        \"healthy\": (150, 50, 50)  # Rouge-rose\n",
    "    }\n",
    "    \n",
    "    # Générer des images pour chaque classe de prune\n",
    "    for cls, base_color in colors.items():\n",
    "        for i in range(10):  # 10 images par classe\n",
    "            # Ajouter un peu de variation aléatoire à la couleur\n",
    "            color_var = [max(0, min(255, c + random.randint(-20, 20))) for c in base_color]\n",
    "            \n",
    "            # Créer une image\n",
    "            from PIL import Image\n",
    "            img = Image.new('RGB', (224, 224), (255, 255, 255))\n",
    "            pixels = img.load()\n",
    "            \n",
    "            # Dessiner un cercle approximatif avec la couleur\n",
    "            center_x, center_y = 112, 112\n",
    "            radius = 100 + random.randint(-10, 10)\n",
    "            \n",
    "            for x in range(img.width):\n",
    "                for y in range(img.height):\n",
    "                    dist = ((x - center_x) ** 2 + (y - center_y) ** 2) ** 0.5\n",
    "                    if dist <= radius:\n",
    "                        # Ajouter du bruit à chaque pixel\n",
    "                        pixel_color = [max(0, min(255, c + random.randint(-10, 10))) for c in color_var]\n",
    "                        pixels[x, y] = tuple(pixel_color)\n",
    "            \n",
    "            # Sauvegarder l'image\n",
    "            img_path = os.path.join(PLUM_DATA_DIR, cls, f\"{cls}_{i+1}.jpg\")\n",
    "            img.save(img_path)\n",
    "    \n",
    "    # Générer des images pour la classe non-prune\n",
    "    for i in range(20):  # 20 images non-prune\n",
    "        # Couleur aléatoire qui n'est pas proche des couleurs de prune\n",
    "        color = (random.randint(0, 100), random.randint(150, 255), random.randint(150, 255))\n",
    "        \n",
    "        # Créer une image\n",
    "        from PIL import Image\n",
    "        img = Image.new('RGB', (224, 224), (255, 255, 255))\n",
    "        pixels = img.load()\n",
    "        \n",
    "        # Dessiner une forme aléatoire (carré ou triangle)\n",
    "        shape = random.choice(['square', 'triangle'])\n",
    "        \n",
    "        if shape == 'square':\n",
    "            # Dessiner un carré\n",
    "            size = random.randint(100, 150)\n",
    "            top_left = (random.randint(0, 224-size), random.randint(0, 224-size))\n",
    "            \n",
    "            for x in range(top_left[0], top_left[0] + size):\n",
    "                for y in range(top_left[1], top_left[1] + size):\n",
    "                    if 0 <= x < 224 and 0 <= y < 224:\n",
    "                        # Ajouter du bruit à chaque pixel\n",
    "                        pixel_color = [max(0, min(255, c + random.randint(-10, 10))) for c in color]\n",
    "                        pixels[x, y] = tuple(pixel_color)\n",
    "        else:\n",
    "            # Dessiner un triangle\n",
    "            p1 = (random.randint(50, 174), random.randint(50, 174))\n",
    "            p2 = (p1[0] + random.randint(30, 50), p1[1] + random.randint(30, 50))\n",
    "            p3 = (p1[0] - random.randint(0, 30), p2[1])\n",
    "            \n",
    "            # Remplir le triangle (algorithme simple)\n",
    "            min_x = min(p1[0], p2[0], p3[0])\n",
    "            max_x = max(p1[0], p2[0], p3[0])\n",
    "            min_y = min(p1[1], p2[1], p3[1])\n",
    "            max_y = max(p1[1], p2[1], p3[1])\n",
    "            \n",
    "            for x in range(min_x, max_x + 1):\n",
    "                for y in range(min_y, max_y + 1):\n",
    "                    if 0 <= x < 224 and 0 <= y < 224:\n",
    "                        # Vérification simple si le point est dans le triangle\n",
    "                        if (x >= p1[0] and y >= p1[1] and x <= p2[0] and y <= p2[1]):\n",
    "                            # Ajouter du bruit à chaque pixel\n",
    "                            pixel_color = [max(0, min(255, c + random.randint(-10, 10))) for c in color]\n",
    "                            pixels[x, y] = tuple(pixel_color)\n",
    "        \n",
    "        # Sauvegarder l'image\n",
    "        img_path = os.path.join(non_plum_dir, f\"non_plum_{i+1}.jpg\")\n",
    "        img.save(img_path)\n",
    "    \n",
    "    print(f\"Données d'exemple créées avec succès!\")\n",
    "    print(f\"- {len(plum_classes)} classes de prunes avec 10 images chacune\")\n",
    "    print(f\"- 20 images non-prune\")\n",
    "\n",
    "# Créer des données d'exemple si nécessaire\n",
    "create_sample_data(force_create=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chargement des données\n",
    "\n",
    "Utilisons la fonction `load_and_prepare_two_stage_data` du module `data_preprocessing` pour charger les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier si les répertoires de données existent et contiennent des images\n",
    "def check_data_availability():\n",
    "    # Vérifier le répertoire des prunes\n",
    "    plum_classes = [d for d in os.listdir(PLUM_DATA_DIR) if os.path.isdir(os.path.join(PLUM_DATA_DIR, d))]\n",
    "    if not plum_classes:\n",
    "        print(f\"Aucune classe de prune trouvée dans {PLUM_DATA_DIR}. Veuillez ajouter des données.\")\n",
    "        return False\n",
    "    \n",
    "    # Vérifier le répertoire des non-prunes\n",
    "    non_plum_dir = os.path.join(NON_PLUM_DATA_DIR, \"non_plum\")\n",
    "    if not os.path.exists(non_plum_dir):\n",
    "        print(f\"Le répertoire {non_plum_dir} n'existe pas. Veuillez créer ce répertoire et y ajouter des images.\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Vérifier la disponibilité des données\n",
    "data_available = check_data_availability()\n",
    "\n",
    "if data_available:\n",
    "    try:\n",
    "        # Charger et préparer les données pour les deux étapes\n",
    "        print(\"Chargement des données pour les deux étapes...\")\n",
    "        (detection_train_loader, detection_val_loader, detection_test_loader, detection_class_names), \\\n",
    "        (classification_train_loader, classification_val_loader, classification_test_loader, classification_class_names) = \\\n",
    "            load_and_prepare_two_stage_data(\n",
    "                PLUM_DATA_DIR, \n",
    "                NON_PLUM_DATA_DIR,\n",
    "                batch_size=BATCH_SIZE, \n",
    "                img_size=IMG_SIZE,\n",
    "                num_workers=NUM_WORKERS\n",
    "            )\n",
    "        \n",
    "        print(f\"Classes de détection: {detection_class_names}\")\n",
    "        print(f\"Classes de classification: {classification_class_names}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement des données: {e}\")\n",
    "else:\n",
    "    print(\"Veuillez d'abord ajouter des données dans les répertoires appropriés.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Création des modèles\n",
    "\n",
    "Utilisons la fonction `get_model` du module `model_architecture` pour créer les modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_available and 'detection_class_names' in locals() and 'classification_class_names' in locals():\n",
    "    # Créer le modèle de détection\n",
    "    detection_model = get_model(\n",
    "        model_name='lightweight', \n",
    "        num_classes=len(detection_class_names), \n",
    "        base_model='mobilenet_v2', \n",
    "        pretrained=True\n",
    "    )\n",
    "    \n",
    "    # Créer le modèle de classification\n",
    "    classification_model = get_model(\n",
    "        model_name='standard', \n",
    "        num_classes=len(classification_class_names),\n",
    "        base_model='resnet18', \n",
    "        pretrained=True\n",
    "    )\n",
    "    \n",
    "    # Afficher les informations sur les modèles\n",
    "    print(\"=== Modèle de détection ===\")\n",
    "    print(f\"Type: {detection_model.__class__.__name__}\")\n",
    "    print(f\"Informations: {detection_model.get_model_info()}\")\n",
    "    \n",
    "    print(\"\\n=== Modèle de classification ===\")\n",
    "    print(f\"Type: {classification_model.__class__.__name__}\")\n",
    "    print(f\"Informations: {classification_model.get_model_info()}\")\n",
    "    \n",
    "    # Déplacer les modèles sur le device\n",
    "    detection_model = detection_model.to(device)\n",
    "    classification_model = classification_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraînement du modèle de détection\n",
    "\n",
    "Utilisons la fonction `train_model` du module `train_two_stage` pour entraîner le modèle de détection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_available and 'detection_train_loader' in locals() and 'detection_val_loader' in locals() and 'detection_model' in locals():\n",
    "    try:\n",
    "        print(\"=== Entraînement du modèle de détection ===\\n\")\n",
    "        \n",
    "        # Définir la fonction de perte et l'optimiseur\n",
    "        detection_criterion = nn.CrossEntropyLoss()\n",
    "        detection_optimizer = optim.Adam(detection_model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # Scheduler pour ajuster le learning rate\n",
    "        detection_scheduler = ReduceLROnPlateau(detection_optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "        \n",
    "        # Entraîner le modèle de détection\n",
    "        detection_history = train_model(\n",
    "            detection_model, \n",
    "            detection_train_loader, \n",
    "            detection_val_loader, \n",
    "            detection_criterion, \n",
    "            detection_optimizer, \n",
    "            detection_scheduler, \n",
    "            device, \n",
    "            num_epochs=NUM_EPOCHS, \n",
    "            early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "            save_dir=MODELS_DIR,\n",
    "            model_name=\"detection\"\n",
    "        )\n",
    "        \n",
    "        # Tracer les courbes d'entraînement\n",
    "        plot_training_history(detection_history, save_dir=MODELS_DIR, model_name=\"detection\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'entraînement du modèle de détection: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entraînement du modèle de classification\n",
    "\n",
    "Utilisons la fonction `train_model` du module `train_two_stage` pour entraîner le modèle de classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_available and 'classification_train_loader' in locals() and 'classification_val_loader' in locals() and 'classification_model' in locals():\n",
    "    try:\n",
    "        print(\"=== Entraînement du modèle de classification ===\\n\")\n",
    "        \n",
    "        # Définir la fonction de perte et l'optimiseur\n",
    "        classification_criterion = nn.CrossEntropyLoss()\n",
    "        classification_optimizer = optim.Adam(classification_model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # Scheduler pour ajuster le learning rate\n",
    "        classification_scheduler = ReduceLROnPlateau(classification_optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "        \n",
    "        # Entraîner le modèle de classification\n",
    "        classification_history = train_model(\n",
    "            classification_model, \n",
    "            classification_train_loader, \n",
    "            classification_val_loader, \n",
    "            classification_criterion, \n",
    "            classification_optimizer, \n",
    "            classification_scheduler, \n",
    "            device, \n",
    "            num_epochs=NUM_EPOCHS, \n",
    "            early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "            save_dir=MODELS_DIR,\n",
    "            model_name=\"classification\"\n",
    "        )\n",
    "        \n",
    "        # Tracer les courbes d'entraînement\n",
    "        plot_training_history(classification_history, save_dir=MODELS_DIR, model_name=\"classification\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'entraînement du modèle de classification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Évaluation des modèles\n",
    "\n",
    "Utilisons la fonction `evaluate_model` du module `train_two_stage` pour évaluer les modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer le modèle de détection\n",
    "if data_available and 'detection_test_loader' in locals() and 'detection_class_names' in locals():\n",
    "    try:\n",
    "        print(\"=== Évaluation du modèle de détection ===\\n\")\n",
    "        \n",
    "        # Charger le meilleur modèle (selon l'accuracy)\n",
    "        detection_model.load_state_dict(torch.load(os.path.join(MODELS_DIR, 'detection_best_acc.pth'), map_location=device))\n",
    "        detection_model = detection_model.to(device)\n",
    "        \n",
    "        # Évaluer le modèle\n",
    "        detection_criterion = nn.CrossEntropyLoss()\n",
    "        detection_metrics = evaluate_model(\n",
    "            detection_model, \n",
    "            detection_test_loader, \n",
    "            detection_criterion, \n",
    "            device, \n",
    "            detection_class_names,\n",
    "            save_dir=MODELS_DIR,\n",
    "            model_name=\"detection\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'évaluation du modèle de détection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer le modèle de classification\n",
    "if data_available and 'classification_test_loader' in locals() and 'classification_class_names' in locals():\n",
    "    try:\n",
    "        print(\"=== Évaluation du modèle de classification ===\\n\")\n",
    "        \n",
    "        # Charger le meilleur modèle (selon l'accuracy)\n",
    "        classification_model.load_state_dict(torch.load(os.path.join(MODELS_DIR, 'classification_best_acc.pth'), map_location=device))\n",
    "        classification_model = classification_model.to(device)\n",
    "        \n",
    "        # Évaluer le modèle\n",
    "        classification_criterion = nn.CrossEntropyLoss()\n",
    "        classification_metrics = evaluate_model(\n",
    "            classification_model, \n",
    "            classification_test_loader, \n",
    "            classification_criterion, \n",
    "            device, \n",
    "            classification_class_names,\n",
    "            save_dir=MODELS_DIR,\n",
    "            model_name=\"classification\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'évaluation du modèle de classification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sauvegarde du modèle à deux étapes\n",
    "\n",
    "Créons et sauvegardons le modèle à deux étapes complet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer et sauvegarder le modèle à deux étapes\n",
    "if data_available and 'detection_class_names' in locals() and 'classification_class_names' in locals():\n",
    "    try:\n",
    "        print(\"=== Création du modèle à deux étapes ===\\n\")\n",
    "        \n",
    "        # Charger les meilleurs modèles\n",
    "        detection_model.load_state_dict(torch.load(os.path.join(MODELS_DIR, 'detection_best_acc.pth'), map_location=device))\n",
    "        classification_model.load_state_dict(torch.load(os.path.join(MODELS_DIR, 'classification_best_acc.pth'), map_location=device))\n",
    "        \n",
    "        # Créer le modèle à deux étapes\n",
    "        two_stage_model = TwoStageModel(detection_model, classification_model, detection_threshold=0.7)\n",
    "        \n",
    "        # Sauvegarder les informations du modèle\n",
    "        model_info = {\n",
    "            'detection_classes': detection_class_names,\n",
    "            'classification_classes': classification_class_names,\n",
    "            'model_info': two_stage_model.get_model_info(),\n",
    "            'img_size': IMG_SIZE,\n",
    "            'date_created': time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(MODELS_DIR, 'two_stage_model_info.json'), 'w') as f:\n",
    "            json.dump(model_info, f, indent=4)\n",
    "        \n",
    "        print(\"Modèle à deux étapes créé et informations sauvegardées.\")\n",
    "        print(f\"Classes de détection: {detection_class_names}\")\n",
    "        print(f\"Classes de classification: {classification_class_names}\")\n",
    "        print(f\"Informations du modèle: {two_stage_model.get_model_info()}\")\n",
    "        \n",
    "        # Dans Colab, permettre de télécharger les modèles entraînés\n",
    "        if IN_COLAB:\n",
    "            from google.colab import files\n",
    "            print(\"\\nVous pouvez télécharger les modèles entraînés ci-dessous:\")\n",
    "            files.download(os.path.join(MODELS_DIR, 'detection_best_acc.pth'))\n",
    "            files.download(os.path.join(MODELS_DIR, 'classification_best_acc.pth'))\n",
    "            files.download(os.path.join(MODELS_DIR, 'two_stage_model_info.json'))\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la création du modèle à deux étapes: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "Dans ce notebook, nous avons utilisé les fonctions existantes des modules `data_preprocessing`, `model_architecture` et `train_two_stage` pour :\n",
    "1. Charger les données\n",
    "2. Créer les modèles de détection et de classification\n",
    "3. Entraîner les modèles\n",
    "4. Évaluer les performances\n",
    "5. Créer et sauvegarder le modèle à deux étapes complet\n",
    "\n",
    "Le modèle est maintenant prêt à être testé sur de nouvelles images dans le notebook suivant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
