{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînement du modèle de classification des prunes africaines\n",
    "\n",
    "Ce notebook utilise les fonctions existantes dans le dépôt pour entraîner le modèle de classification des prunes africaines en utilisant le jeu de données Kaggle \"African Plums Dataset\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration de l'environnement pour Google Colab\n",
    "\n",
    "Commençons par cloner le dépôt GitHub et configurer l'environnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier si nous sommes dans Google Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"Exécution dans Google Colab: {IN_COLAB}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Cloner le dépôt GitHub\n",
    "    !git clone https://github.com/CodeStorm-mbe/african-plums-classifier.git\n",
    "    %cd african-plums-classifier\n",
    "    \n",
    "    # Installer les dépendances requises\n",
    "    !pip install -r requirements.txt\n",
    "    !pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration de l'API Kaggle\n",
    "\n",
    "Pour télécharger le jeu de données Kaggle, nous devons configurer l'API Kaggle si ce n'est pas déjà fait dans le notebook de préparation des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "# Ajouter le répertoire courant au chemin pour pouvoir importer nos modules\n",
    "if IN_COLAB:\n",
    "    # Dans Colab, nous sommes déjà dans le répertoire du projet\n",
    "    if \"/content/african-plums-classifier\" not in sys.path:\n",
    "        sys.path.append(\"/content/african-plums-classifier\")\n",
    "else:\n",
    "    # En local, ajouter le répertoire parent\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "# Importer nos modules personnalisés\n",
    "from data.data_preprocessing import load_and_prepare_two_stage_data\n",
    "from models.model_architecture import get_model, TwoStageModel\n",
    "from scripts.train_two_stage import train_model, evaluate_model, plot_training_history\n",
    "\n",
    "# Définir les chemins des données\n",
    "if IN_COLAB:\n",
    "    # Dans Colab, créer les répertoires dans le dossier du projet cloné\n",
    "    DATA_ROOT = \"data/raw\"\n",
    "    KAGGLE_DIR = \"data/kaggle\"\n",
    "    MODELS_DIR = \"models/saved\"\n",
    "else:\n",
    "    # En local\n",
    "    DATA_ROOT = \"../data/raw\"\n",
    "    KAGGLE_DIR = \"../data/kaggle\"\n",
    "    MODELS_DIR = \"../models/saved\"\n",
    "\n",
    "PLUM_DATA_DIR = os.path.join(DATA_ROOT, \"plums\")  # Sous-dossier pour les prunes\n",
    "NON_PLUM_DATA_DIR = os.path.join(DATA_ROOT, \"non_plums\")  # Sous-dossier pour les non-prunes\n",
    "\n",
    "# Créer les répertoires s'ils n'existent pas\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(PLUM_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(NON_PLUM_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(KAGGLE_DIR, exist_ok=True)\n",
    "\n",
    "# Définir les paramètres d'entraînement\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 224\n",
    "NUM_WORKERS = 2 if IN_COLAB else 4  # Réduire le nombre de workers dans Colab\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10 if IN_COLAB else 25  # Réduire le nombre d'époques dans Colab pour accélérer\n",
    "EARLY_STOPPING_PATIENCE = 3 if IN_COLAB else 7  # Réduire la patience dans Colab\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Fixer les seeds pour la reproductibilité\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Déterminer le device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilisation de: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de l'API Kaggle si nécessaire\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    \n",
    "    # Vérifier si le fichier kaggle.json existe déjà\n",
    "    kaggle_config_exists = os.path.exists(os.path.expanduser('~/.kaggle/kaggle.json'))\n",
    "    \n",
    "    if not kaggle_config_exists:\n",
    "        print(\"Veuillez télécharger votre fichier kaggle.json pour l'authentification Kaggle.\")\n",
    "        print(\"Vous pouvez le générer sur https://www.kaggle.com/account dans la section 'API'.\")\n",
    "        \n",
    "        # Télécharger le fichier kaggle.json\n",
    "        uploaded = files.upload()\n",
    "        \n",
    "        # Créer le répertoire .kaggle s'il n'existe pas\n",
    "        os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n",
    "        \n",
    "        # Déplacer le fichier kaggle.json vers le répertoire .kaggle\n",
    "        if 'kaggle.json' in uploaded:\n",
    "            shutil.move('kaggle.json', os.path.expanduser('~/.kaggle/kaggle.json'))\n",
    "            # Définir les permissions appropriées\n",
    "            os.chmod(os.path.expanduser('~/.kaggle/kaggle.json'), 600)\n",
    "            print(\"Fichier kaggle.json configuré avec succès.\")\n",
    "        else:\n",
    "            print(\"Erreur: Le fichier kaggle.json n'a pas été téléchargé.\")\n",
    "    else:\n",
    "        print(\"Le fichier kaggle.json existe déjà.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vérification et préparation des données\n",
    "\n",
    "Vérifions si les données du jeu de données Kaggle \"African Plums Dataset\" sont déjà préparées. Si ce n'est pas le cas, nous les téléchargerons et les préparerons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def download_kaggle_dataset(force_download=False):\n",
    "    \"\"\"Télécharge le jeu de données Kaggle 'African Plums Dataset'.\"\"\"\n",
    "    # Vérifier si le jeu de données a déjà été téléchargé\n",
    "    dataset_zip = os.path.join(KAGGLE_DIR, 'african-plums-dataset.zip')\n",
    "    if os.path.exists(dataset_zip) and not force_download:\n",
    "        print(f\"Le jeu de données a déjà été téléchargé dans {dataset_zip}.\")\n",
    "        return dataset_zip\n",
    "    \n",
    "    print(\"Téléchargement du jeu de données Kaggle 'African Plums Dataset'...\")\n",
    "    try:\n",
    "        # Télécharger le jeu de données\n",
    "        !kaggle datasets download -d arnaudfadja/african-plums-quality-and-defect-assessment-data -p {KAGGLE_DIR}\n",
    "        print(f\"Jeu de données téléchargé avec succès dans {dataset_zip}.\")\n",
    "        return dataset_zip\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du téléchargement du jeu de données: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_and_organize_dataset(dataset_zip, force_extract=False):\n",
    "    \"\"\"Extrait et organise le jeu de données Kaggle pour notre modèle.\"\"\"\n",
    "    if not os.path.exists(dataset_zip):\n",
    "        print(f\"Le fichier {dataset_zip} n'existe pas.\")\n",
    "        return False\n",
    "    \n",
    "    # Vérifier si les données ont déjà été extraites\n",
    "    extracted_dir = os.path.join(KAGGLE_DIR, 'extracted')\n",
    "    if os.path.exists(extracted_dir) and not force_extract:\n",
    "        print(f\"Le jeu de données a déjà été extrait dans {extracted_dir}.\")\n",
    "    else:\n",
    "        print(f\"Extraction du jeu de données...\")\n",
    "        os.makedirs(extracted_dir, exist_ok=True)\n",
    "        \n",
    "        # Extraire le fichier zip\n",
    "        with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extracted_dir)\n",
    "        \n",
    "        print(f\"Jeu de données extrait avec succès dans {extracted_dir}.\")\n",
    "    \n",
    "    # Organiser les données pour notre modèle\n",
    "    print(\"Organisation des données pour notre modèle...\")\n",
    "    \n",
    "    # Vérifier la structure du jeu de données extrait\n",
    "    print(\"Structure du jeu de données extrait:\")\n",
    "    !find {extracted_dir} -type d | sort\n",
    "    \n",
    "    # Mapper les classes du jeu de données Kaggle aux classes de notre modèle\n",
    "    # Selon la description, les classes sont: bruised, cracked, rotten, spotted, unaffected, unripe\n",
    "    class_mapping = {\n",
    "        'bruised': 'bruised',\n",
    "        'cracked': 'cracked',\n",
    "        'rotten': 'rotten',\n",
    "        'spotted': 'spotted',\n",
    "        'unaffected': 'unaffected',\n",
    "        'unripe': 'unripe'\n",
    "    }\n",
    "    \n",
    "    # Créer les répertoires pour les classes de prunes\n",
    "    for cls in class_mapping.values():\n",
    "        os.makedirs(os.path.join(PLUM_DATA_DIR, cls), exist_ok=True)\n",
    "    \n",
    "    # Créer le répertoire pour les non-prunes\n",
    "    non_plum_dir = os.path.join(NON_PLUM_DATA_DIR, \"non_plum\")\n",
    "    os.makedirs(non_plum_dir, exist_ok=True)\n",
    "    \n",
    "    # Copier les images dans les répertoires appropriés\n",
    "    dataset_dir = os.path.join(extracted_dir, 'african_plums_dataset')\n",
    "    if os.path.exists(dataset_dir):\n",
    "        # Parcourir les sous-répertoires du jeu de données\n",
    "        for src_cls, dst_cls in class_mapping.items():\n",
    "            src_dir = os.path.join(dataset_dir, src_cls)\n",
    "            dst_dir = os.path.join(PLUM_DATA_DIR, dst_cls)\n",
    "            \n",
    "            if os.path.exists(src_dir):\n",
    "                # Copier les images\n",
    "                print(f\"Copie des images de {src_dir} vers {dst_dir}...\")\n",
    "                !cp -r {src_dir}/* {dst_dir}/\n",
    "            else:\n",
    "                print(f\"Le répertoire {src_dir} n'existe pas.\")\n",
    "        \n",
    "        print(\"Images copiées avec succès.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Le répertoire {dataset_dir} n'existe pas.\")\n",
    "        return False\n",
    "\n",
    "def download_non_plum_images(num_images=100):\n",
    "    \"\"\"Télécharge des images non-prunes à partir d'un autre jeu de données Kaggle.\"\"\"\n",
    "    non_plum_dir = os.path.join(NON_PLUM_DATA_DIR, \"non_plum\")\n",
    "    \n",
    "    # Vérifier si des images non-prunes existent déjà\n",
    "    existing_images = [f for f in os.listdir(non_plum_dir) if os.path.isfile(os.path.join(non_plum_dir, f))]\n",
    "    if existing_images:\n",
    "        print(f\"Des images non-prunes existent déjà ({len(existing_images)} images).\")\n",
    "        return\n",
    "    \n",
    "    print(\"Téléchargement d'images non-prunes...\")\n",
    "    \n",
    "    # Option 1: Télécharger des images de fruits (autres que des prunes) à partir d'un jeu de données Kaggle\n",
    "    try:\n",
    "        # Télécharger un jeu de données de fruits\n",
    "        !kaggle datasets download -d moltean/fruits -p {KAGGLE_DIR}\n",
    "        \n",
    "        # Extraire le jeu de données\n",
    "        fruits_zip = os.path.join(KAGGLE_DIR, 'fruits.zip')\n",
    "        fruits_dir = os.path.join(KAGGLE_DIR, 'fruits')\n",
    "        os.makedirs(fruits_dir, exist_ok=True)\n",
    "        \n",
    "        with zipfile.ZipFile(fruits_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(fruits_dir)\n",
    "        \n",
    "        # Sélectionner des images aléatoires (excluant les prunes)\n",
    "        import glob\n",
    "        all_fruit_images = []\n",
    "        for fruit_dir in glob.glob(os.path.join(fruits_dir, 'fruits-360/Training/*')):\n",
    "            fruit_name = os.path.basename(fruit_dir).lower()\n",
    "            if 'plum' not in fruit_name and 'prune' not in fruit_name:\n",
    "                all_fruit_images.extend(glob.glob(os.path.join(fruit_dir, '*.jpg')))\n",
    "        \n",
    "        # Sélectionner un sous-ensemble aléatoire\n",
    "        if all_fruit_images:\n",
    "            selected_images = random.sample(all_fruit_images, min(num_images, len(all_fruit_images)))\n",
    "            \n",
    "            # Copier les images sélectionnées\n",
    "            for i, img_path in enumerate(selected_images):\n",
    "                dst_path = os.path.join(non_plum_dir, f\"non_plum_{i+1}.jpg\")\n",
    "                shutil.copy(img_path, dst_path)\n",
    "            \n",
    "            print(f\"{len(selected_images)} images non-prunes copiées avec succès.\")\n",
    "        else:\n",
    "            print(\"Aucune image de fruit trouvée.\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du téléchargement d'images non-prunes: {e}\")\n",
    "        \n",
    "        # Option 2: Créer des images synthétiques si le téléchargement échoue\n",
    "        print(\"Création d'images non-prunes synthétiques...\")\n",
    "        \n",
    "        for i in range(num_images):\n",
    "            # Couleur aléatoire qui n'est pas proche des couleurs de prune\n",
    "            color = (random.randint(0, 100), random.randint(150, 255), random.randint(150, 255))\n",
    "            \n",
    "            # Créer une image\n",
    "            from PIL import Image\n",
    "            img = Image.new('RGB', (224, 224), (255, 255, 255))\n",
    "            pixels = img.load()\n",
    "            \n",
    "            # Dessiner une forme aléatoire (carré ou triangle)\n",
    "            shape = random.choice(['square', 'triangle'])\n",
    "            \n",
    "            if shape == 'square':\n",
    "                # Dessiner un carré\n",
    "                size = random.randint(100, 150)\n",
    "                top_left = (random.randint(0, 224-size), random.randint(0, 224-size))\n",
    "                \n",
    "                for x in range(top_left[0], top_left[0] + size):\n",
    "                    for y in range(top_left[1], top_left[1] + size):\n",
    "                        if 0 <= x < 224 and 0 <= y < 224:\n",
    "                            # Ajouter du bruit à chaque pixel\n",
    "                            pixel_color = [max(0, min(255, c + random.randint(-10, 10))) for c in color]\n",
    "                            pixels[x, y] = tuple(pixel_color)\n",
    "            else:\n",
    "                # Dessiner un triangle\n",
    "                p1 = (random.randint(50, 174), random.randint(50, 174))\n",
    "                p2 = (p1[0] + random.randint(30, 50), p1[1] + random.randint(30, 50))\n",
    "                p3 = (p1[0] - random.randint(0, 30), p2[1])\n",
    "                \n",
    "                # Remplir le triangle (algorithme simple)\n",
    "                min_x = min(p1[0], p2[0], p3[0])\n",
    "                max_x = max(p1[0], p2[0], p3[0])\n",
    "                min_y = min(p1[1], p2[1], p3[1])\n",
    "                max_y = max(p1[1], p2[1], p3[1])\n",
    "                \n",
    "                for x in range(min_x, max_x + 1):\n",
    "                    for y in range(min_y, max_y + 1):\n",
    "                        if 0 <= x < 224 and 0 <= y < 224:\n",
    "                            # Vérification simple si le point est dans le triangle\n",
    "                            if (x >= p1[0] and y >= p1[1] and x <= p2[0] and y <= p2[1]):\n",
    "                                # Ajouter du bruit à chaque pixel\n",
    "                                pixel_color = [max(0, min(255, c + random.randint(-10, 10))) for c in color]\n",
    "                                pixels[x, y] = tuple(pixel_color)\n",
    "            \n",
    "            # Sauvegarder l'image\n",
    "            img_path = os.path.join(non_plum_dir, f\"non_plum_{i+1}.jpg\")\n",
    "            img.save(img_path)\n",
    "        \n",
    "        print(f\"{num_images} images non-prunes synthétiques créées avec succès.\")\n",
    "        return True\n",
    "\n",
    "# Vérifier si les données sont déjà préparées\n",
    "def check_data_availability():\n",
    "    # Vérifier le répertoire des prunes\n",
    "    plum_classes = [d for d in os.listdir(PLUM_DATA_DIR) if os.path.isdir(os.path.join(PLUM_DATA_DIR, d))]\n",
    "    if not plum_classes:\n",
    "        print(f\"Aucune classe de prune trouvée dans {PLUM_DATA_DIR}. Téléchargement des données nécessaire.\")\n",
    "        return False\n",
    "    \n",
    "    # Vérifier le répertoire des non-prunes\n",
    "    non_plum_dir = os.path.join(NON_PLUM_DATA_DIR, \"non_plum\")\n",
    "    if not os.path.exists(non_plum_dir):\n",
    "        print(f\"Le répertoire {non_plum_dir} n'existe pas. Téléchargement des données nécessaire.\")\n",
    "        return False\n",
    "    \n",
    "    # Vérifier s'il y a des images dans les répertoires\n",
    "    plum_images_count = sum(len([f for f in os.listdir(os.path.join(PLUM_DATA_DIR, cls)) \n",
    "                                if os.path.isfile(os.path.join(PLUM_DATA_DIR, cls, f))]) for cls in plum_classes)\n",
    "    non_plum_images_count = len([f for f in os.listdir(non_plum_dir) if os.path.isfile(os.path.join(non_plum_dir, f))])\n",
    "    \n",
    "    if plum_images_count == 0 or non_plum_images_count == 0:\n",
    "        print(f\"Pas assez d'images dans les répertoires. Téléchargement des données nécessaire.\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"Données disponibles: {plum_images_count} images de prunes dans {len(plum_classes)} classes, {non_plum_images_count} images non-prunes.\")\n",
    "    return True\n",
    "\n",
    "# Vérifier et préparer les données si nécessaire\n",
    "data_available = check_data_availability()\n",
    "\n",
    "if not data_available:\n",
    "    print(\"Préparation des données...\")\n",
    "    # Télécharger et préparer le jeu de données\n",
    "    dataset_zip = download_kaggle_dataset(force_download=False)\n",
    "    if dataset_zip:\n",
    "        success = extract_and_organize_dataset(dataset_zip, force_extract=False)\n",
    "        if success:\n",
    "            print(\"Jeu de données Kaggle préparé avec succès pour notre modèle.\")\n",
    "            # Télécharger des images non-prunes\n",
    "            download_non_plum_images(num_images=100)\n",
    "            data_available = check_data_availability()\n",
    "        else:\n",
    "            print(\"Erreur lors de la préparation du jeu de données Kaggle.\")\n",
    "    else:\n",
    "        print(\"Erreur lors du téléchargement du jeu de données Kaggle.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chargement des données\n",
    "\n",
    "Utilisons la fonction `load_and_prepare_two_stage_data` du module `data_preprocessing` pour charger les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_available:\n",
    "    try:\n",
    "        # Charger et préparer les données pour les deux étapes\n",
    "        print(\"Chargement des données pour les deux étapes...\")\n",
    "        (detection_train_loader, detection_val_loader, detection_test_loader, detection_class_names), \\\n",
    "        (classification_train_loader, classification_val_loader, classification_test_loader, classification_class_names) = \\\n",
    "            load_and_prepare_two_stage_data(\n",
    "                PLUM_DATA_DIR, \n",
    "                NON_PLUM_DATA_DIR,\n",
    "                batch_size=BATCH_SIZE, \n",
    "                img_size=IMG_SIZE,\n",
    "                num_workers=NUM_WORKERS\n",
    "            )\n",
    "        \n",
    "        print(f\"Classes de détection: {detection_class_names}\")\n",
    "        print(f\"Classes de classification: {classification_class_names}\")\n",
    "        \n",
    "        # Afficher les tailles des datasets\n",
    "        print(f\"\\nTailles des datasets de détection:\")\n",
    "        print(f\"  - Entraînement: {len(detection_train_loader.dataset)} images\")\n",
    "        print(f\"  - Validation: {len(detection_val_loader.dataset)} images\")\n",
    "        print(f\"  - Test: {len(detection_test_loader.dataset)} images\")\n",
    "        \n",
    "        print(f\"\\nTailles des datasets de classification:\")\n",
    "        print(f\"  - Entraînement: {len(classification_train_loader.dataset)} images\")\n",
    "        print(f\"  - Validation: {len(classification_val_loader.dataset)} images\")\n",
    "        print(f\"  - Test: {len(classification_test_loader.dataset)} images\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement des données: {e}\")\n",
    "else:\n",
    "    print(\"Veuillez d'abord préparer les données.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Création des modèles\n",
    "\n",
    "Utilisons la fonction `get_model` du module `model_architecture` pour créer les modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_available and 'detection_class_names' in locals() and 'classification_class_names' in locals():\n",
    "    # Créer le modèle de détection\n",
    "    detection_model = get_model(\n",
    "        model_name='lightweight', \n",
    "        num_classes=len(detection_class_names), \n",
    "        base_model='mobilenet_v2', \n",
    "        pretrained=True\n",
    "    )\n",
    "    \n",
    "    # Créer le modèle de classification\n",
    "    classification_model = get_model(\n",
    "        model_name='standard', \n",
    "        num_classes=len(classification_class_names),\n",
    "        base_model='resnet18', \n",
    "        pretrained=True\n",
    "    )\n",
    "    \n",
    "    # Afficher les informations sur les modèles\n",
    "    print(\"=== Modèle de détection ===\")\n",
    "    print(f\"Type: {detection_model.__class__.__name__}\")\n",
    "    print(f\"Informations: {detection_model.get_model_info()}\")\n",
    "    \n",
    "    print(\"\\n=== Modèle de classification ===\")\n",
    "    print(f\"Type: {classification_model.__class__.__name__}\")\n",
    "    print(f\"Informations: {classification_model.get_model_info()}\")\n",
    "    \n",
    "    # Déplacer les modèles sur le device\n",
    "    detection_model = detection_model.to(device)\n",
    "    classification_model = classification_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entraînement du modèle de détection\n",
    "\n",
    "Utilisons la fonction `train_model` du module `train_two_stage` pour entraîner le modèle de détection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_available and 'detection_train_loader' in locals() and 'detection_val_loader' in locals() and 'detection_model' in locals():\n",
    "    try:\n",
    "        print(\"=== Entraînement du modèle de détection ===\\n\")\n",
    "        \n",
    "        # Définir la fonction de perte et l'optimiseur\n",
    "        detection_criterion = nn.CrossEntropyLoss()\n",
    "        detection_optimizer = optim.Adam(detection_model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # Scheduler pour ajuster le learning rate\n",
    "        detection_scheduler = ReduceLROnPlateau(detection_optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "        \n",
    "        # Entraîner le modèle de détection\n",
    "        detection_history = train_model(\n",
    "            detection_model, \n",
    "            detection_train_loader, \n",
    "            detection_val_loader, \n",
    "            detection_criterion, \n",
    "            detection_optimizer, \n",
    "            detection_scheduler, \n",
    "            device, \n",
    "            num_epochs=NUM_EPOCHS, \n",
    "            early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "            save_dir=MODELS_DIR,\n",
    "            model_name=\"detection\"\n",
    "        )\n",
    "        \n",
    "        # Tracer les courbes d'entraînement\n",
    "        plot_training_history(detection_history, save_dir=MODELS_DIR, model_name=\"detection\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'entraînement du modèle de détection: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Entraînement du modèle de classification\n",
    "\n",
    "Utilisons la fonction `train_model` du module `train_two_stage` pour entraîner le modèle de classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_available and 'classification_train_loader' in locals() and 'classification_val_loader' in locals() and 'classification_model' in locals():\n",
    "    try:\n",
    "        print(\"=== Entraînement du modèle de classification ===\\n\")\n",
    "        \n",
    "        # Définir la fonction de perte et l'optimiseur\n",
    "        classification_criterion = nn.CrossEntropyLoss()\n",
    "        classification_optimizer = optim.Adam(classification_model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # Scheduler pour ajuster le learning rate\n",
    "        classification_scheduler = ReduceLROnPlateau(classification_optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "        \n",
    "        # Entraîner le modèle de classification\n",
    "        classification_history = train_model(\n",
    "            classification_model, \n",
    "            classification_train_loader, \n",
    "            classification_val_loader, \n",
    "            classification_criterion, \n",
    "            classification_optimizer, \n",
    "            classification_scheduler, \n",
    "            device, \n",
    "            num_epochs=NUM_EPOCHS, \n",
    "            early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "            save_dir=MODELS_DIR,\n",
    "            model_name=\"classification\"\n",
    "        )\n",
    "        \n",
    "        # Tracer les courbes d'entraînement\n",
    "        plot_training_history(classification_history, save_dir=MODELS_DIR, model_name=\"classification\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'entraînement du modèle de classification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Évaluation des modèles\n",
    "\n",
    "Utilisons la fonction `evaluate_model` du module `train_two_stage` pour évaluer les modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer le modèle de détection\n",
    "if data_available and 'detection_test_loader' in locals() and 'detection_class_names' in locals():\n",
    "    try:\n",
    "        print(\"=== Évaluation du modèle de détection ===\\n\")\n",
    "        \n",
    "        # Charger le meilleur modèle (selon l'accuracy)\n",
    "        detection_model.load_state_dict(torch.load(os.path.join(MODELS_DIR, 'detection_best_acc.pth'), map_location=device))\n",
    "        detection_model = detection_model.to(device)\n",
    "        \n",
    "        # Évaluer le modèle\n",
    "        detection_criterion = nn.CrossEntropyLoss()\n",
    "        detection_metrics = evaluate_model(\n",
    "            detection_model, \n",
    "            detection_test_loader, \n",
    "            detection_criterion, \n",
    "            device, \n",
    "            detection_class_names,\n",
    "            save_dir=MODELS_DIR,\n",
    "            model_name=\"detection\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'évaluation du modèle de détection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer le modèle de classification\n",
    "if data_available and 'classification_test_loader' in locals() and 'classification_class_names' in locals():\n",
    "    try:\n",
    "        print(\"=== Évaluation du modèle de classification ===\\n\")\n",
    "        \n",
    "        # Charger le meilleur modèle (selon l'accuracy)\n",
    "        classification_model.load_state_dict(torch.load(os.path.join(MODELS_DIR, 'classification_best_acc.pth'), map_location=device))\n",
    "        classification_model = classification_model.to(device)\n",
    "        \n",
    "        # Évaluer le modèle\n",
    "        classification_criterion = nn.CrossEntropyLoss()\n",
    "        classification_metrics = evaluate_model(\n",
    "            classification_model, \n",
    "            classification_test_loader, \n",
    "            classification_criterion, \n",
    "            device, \n",
    "            classification_class_names,\n",
    "            save_dir=MODELS_DIR,\n",
    "            model_name=\"classification\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'évaluation du modèle de classification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sauvegarde du modèle à deux étapes\n",
    "\n",
    "Créons et sauvegardons le modèle à deux étapes complet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer et sauvegarder le modèle à deux étapes\n",
    "if data_available and 'detection_class_names' in locals() and 'classification_class_names' in locals():\n",
    "    try:\n",
    "        print(\"=== Création du modèle à deux étapes ===\\n\")\n",
    "        \n",
    "        # Charger les meilleurs modèles\n",
    "        detection_model.load_state_dict(torch.load(os.path.join(MODELS_DIR, 'detection_best_acc.pth'), map_location=device))\n",
    "        classification_model.load_state_dict(torch.load(os.path.join(MODELS_DIR, 'classification_best_acc.pth'), map_location=device))\n",
    "        \n",
    "        # Créer le modèle à deux étapes\n",
    "        two_stage_model = TwoStageModel(detection_model, classification_model, detection_threshold=0.7)\n",
    "        \n",
    "        # Sauvegarder les informations du modèle\n",
    "        model_info = {\n",
    "            'detection_classes': detection_class_names,\n",
    "            'classification_classes': classification_class_names,\n",
    "            'model_info': two_stage_model.get_model_info(),\n",
    "            'img_size': IMG_SIZE,\n",
    "            'date_created': time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(MODELS_DIR, 'two_stage_model_info.json'), 'w') as f:\n",
    "            json.dump(model_info, f, indent=4)\n",
    "        \n",
    "        print(\"Modèle à deux étapes créé et informations sauvegardées.\")\n",
    "        print(f\"Classes de détection: {detection_class_names}\")\n",
    "        print(f\"Classes de classification: {classification_class_names}\")\n",
    "        print(f\"Informations du modèle: {two_stage_model.get_model_info()}\")\n",
    "        \n",
    "        # Dans Colab, permettre de télécharger les modèles entraînés\n",
    "        if IN_COLAB:\n",
    "            from google.colab import files\n",
    "            print(\"\\nVous pouvez télécharger les modèles entraînés ci-dessous:\")\n",
    "            files.download(os.path.join(MODELS_DIR, 'detection_best_acc.pth'))\n",
    "            files.download(os.path.join(MODELS_DIR, 'classification_best_acc.pth'))\n",
    "            files.download(os.path.join(MODELS_DIR, 'two_stage_model_info.json'))\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la création du modèle à deux étapes: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "Dans ce notebook, nous avons utilisé les fonctions existantes des modules `data_preprocessing`, `model_architecture` et `train_two_stage` pour :\n",
    "1. Télécharger et préparer le jeu de données Kaggle \"African Plums Dataset\"\n",
    "2. Charger les données\n",
    "3. Créer les modèles de détection et de classification\n",
    "4. Entraîner les modèles\n",
    "5. Évaluer les performances\n",
    "6. Créer et sauvegarder le modèle à deux étapes complet\n",
    "\n",
    "Le modèle est maintenant prêt à être testé sur de nouvelles images dans le notebook suivant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
