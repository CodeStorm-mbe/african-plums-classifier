{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînement du modèle de classification des prunes africaines\n",
    "\n",
    "Ce notebook utilise les fonctions existantes dans le dépôt pour entraîner le modèle de classification des prunes africaines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Ajouter le répertoire parent au chemin pour pouvoir importer nos modules\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Importer nos modules personnalisés\n",
    "from data.data_preprocessing import load_and_prepare_two_stage_data\n",
    "from models.model_architecture import get_model\n",
    "from scripts.train_two_stage import train_model, evaluate_model, plot_training_history\n",
    "\n",
    "# Définir les chemins des données\n",
    "DATA_ROOT = \"../data/raw\"  # Chemin vers le répertoire de données brutes\n",
    "PLUM_DATA_DIR = os.path.join(DATA_ROOT, \"plums\")  # Sous-dossier pour les prunes\n",
    "NON_PLUM_DATA_DIR = os.path.join(DATA_ROOT, \"non_plums\")  # Sous-dossier pour les non-prunes\n",
    "MODELS_DIR = \"../models/saved\"  # Répertoire pour sauvegarder les modèles entraînés\n",
    "\n",
    "# Créer les répertoires s'ils n'existent pas\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(PLUM_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(NON_PLUM_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Définir les paramètres d'entraînement\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 224\n",
    "NUM_WORKERS = 4\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 25\n",
    "EARLY_STOPPING_PATIENCE = 7\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Fixer les seeds pour la reproductibilité\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Déterminer le device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilisation de: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des données\n",
    "\n",
    "Utilisons la fonction `load_and_prepare_two_stage_data` du module `data_preprocessing` pour charger les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier si les répertoires de données existent et contiennent des images\n",
    "def check_data_availability():\n",
    "    # Vérifier le répertoire des prunes\n",
    "    plum_classes = [d for d in os.listdir(PLUM_DATA_DIR) if os.path.isdir(os.path.join(PLUM_DATA_DIR, d))]\n",
    "    if not plum_classes:\n",
    "        print(f\"Aucune classe de prune trouvée dans {PLUM_DATA_DIR}. Veuillez ajouter des données.\")\n",
    "        return False\n",
    "    \n",
    "    # Vérifier le répertoire des non-prunes\n",
    "    non_plum_dir = os.path.join(NON_PLUM_DATA_DIR, \"non_plum\")\n",
    "    if not os.path.exists(non_plum_dir):\n",
    "        print(f\"Le répertoire {non_plum_dir} n'existe pas. Veuillez créer ce répertoire et y ajouter des images.\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Vérifier la disponibilité des données\n",
    "data_available = check_data_availability()\n",
    "\n",
    "if data_available:\n",
    "    try:\n",
    "        # Charger et préparer les données pour les deux étapes\n",
    "        print(\"Chargement des données pour les deux étapes...\")\n",
    "        (detection_train_loader, detection_val_loader, detection_test_loader, detection_class_names), \\\n",
    "        (classification_train_loader, classification_val_loader, classification_test_loader, classification_class_names) = \\\n",
    "            load_and_prepare_two_stage_data(\n",
    "                PLUM_DATA_DIR, \n",
    "                NON_PLUM_DATA_DIR,\n",
    "                batch_size=BATCH_SIZE, \n",
    "                img_size=IMG_SIZE,\n",
    "                num_workers=NUM_WORKERS\n",
    "            )\n",
    "        \n",
    "        print(f\"Classes de détection: {detection_class_names}\")\n",
    "        print(f\"Classes de classification: {classification_class_names}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement des données: {e}\")\n",
    "else:\n",
    "    print(\"Veuillez d'abord ajouter des données dans les répertoires appropriés.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Création des modèles\n",
    "\n",
    "Utilisons la fonction `get_model` du module `model_architecture` pour créer les modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_available and 'detection_class_names' in locals() and 'classification_class_names' in locals():\n",
    "    # Créer le modèle de détection\n",
    "    detection_model = get_model(\n",
    "        model_name='lightweight', \n",
    "        num_classes=len(detection_class_names), \n",
    "        base_model='mobilenet_v2', \n",
    "        pretrained=True\n",
    "    )\n",
    "    \n",
    "    # Créer le modèle de classification\n",
    "    classification_model = get_model(\n",
    "        model_name='standard', \n",
    "        num_classes=len(classification_class_names),\n",
    "        base_model='resnet18', \n",
    "        pretrained=True\n",
    "    )\n",
    "    \n",
    "    # Afficher les informations sur les modèles\n",
    "    print(\"=== Modèle de détection ===\")\n",
    "    print(f\"Type: {detection_model.__class__.__name__}\")\n",
    "    print(f\"Informations: {detection_model.get_model_info()}\")\n",
    "    \n",
    "    print(\"\\n=== Modèle de classification ===\")\n",
    "    print(f\"Type: {classification_model.__class__.__name__}\")\n",
    "    print(f\"Informations: {classification_model.get_model_info()}\")\n",
    "    \n",
    "    # Déplacer les modèles sur le device\n",
    "    detection_model = detection_model.to(device)\n",
    "    classification_model = classification_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entraînement du modèle de détection\n",
    "\n",
    "Utilisons la fonction `train_model` du module `train_two_stage` pour entraîner le modèle de détection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_available and 'detection_train_loader' in locals() and 'detection_val_loader' in locals() and 'detection_model' in locals():\n",
    "    try:\n",
    "        print(\"=== Entraînement du modèle de détection ===\\n\")\n",
    "        \n",
    "        # Définir la fonction de perte et l'optimiseur\n",
    "        detection_criterion = nn.CrossEntropyLoss()\n",
    "        detection_optimizer = optim.Adam(detection_model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # Scheduler pour ajuster le learning rate\n",
    "        detection_scheduler = ReduceLROnPlateau(detection_optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "        \n",
    "        # Entraîner le modèle de détection\n",
    "        detection_history = train_model(\n",
    "            detection_model, \n",
    "            detection_train_loader, \n",
    "            detection_val_loader, \n",
    "            detection_criterion, \n",
    "            detection_optimizer, \n",
    "            detection_scheduler, \n",
    "            device, \n",
    "            num_epochs=NUM_EPOCHS, \n",
    "            early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "            save_dir=MODELS_DIR,\n",
    "            model_name=\"detection\"\n",
    "        )\n",
    "        \n",
    "        # Tracer les courbes d'entraînement\n",
    "        plot_training_history(detection_history, save_dir=MODELS_DIR, model_name=\"detection\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'entraînement du modèle de détection: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraînement du modèle de classification\n",
    "\n",
    "Utilisons la fonction `train_model` du module `train_two_stage` pour entraîner le modèle de classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_available and 'classification_train_loader' in locals() and 'classification_val_loader' in locals() and 'classification_model' in locals():\n",
    "    try:\n",
    "        print(\"=== Entraînement du modèle de classification ===\\n\")\n",
    "        \n",
    "        # Définir la fonction de perte et l'optimiseur\n",
    "        classification_criterion = nn.CrossEntropyLoss()\n",
    "        classification_optimizer = optim.Adam(classification_model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # Scheduler pour ajuster le learning rate\n",
    "        classification_scheduler = ReduceLROnPlateau(classification_optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "        \n",
    "        # Entraîner le modèle de classification\n",
    "        classification_history = train_model(\n",
    "            classification_model, \n",
    "            classification_train_loader, \n",
    "            classification_val_loader, \n",
    "            classification_criterion, \n",
    "            classification_optimizer, \n",
    "            classification_scheduler, \n",
    "            device, \n",
    "            num_epochs=NUM_EPOCHS, \n",
    "            early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "            save_dir=MODELS_DIR,\n",
    "            model_name=\"classification\"\n",
    "        )\n",
    "        \n",
    "        # Tracer les courbes d'entraînement\n",
    "        plot_training_history(classification_history, save_dir=MODELS_DIR, model_name=\"classification\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'entraînement du modèle de classification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Évaluation des modèles\n",
    "\n",
    "Utilisons la fonction `evaluate_model` du module `train_two_stage` pour évaluer les modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer le modèle de détection\n",
    "if data_available and 'detection_test_loader' in locals() and 'detection_class_names' in locals():\n",
    "    try:\n",
    "        print(\"=== Évaluation du modèle de détection ===\\n\")\n",
    "        \n",
    "        # Charger le meilleur modèle (selon l'accuracy)\n",
    "        detection_model.load_state_dict(torch.load(os.path.join(MODELS_DIR, 'detection_best_acc.pth')))\n",
    "        detection_model = detection_model.to(device)\n",
    "        \n",
    "        # Évaluer le modèle\n",
    "        detection_criterion = nn.CrossEntropyLoss()\n",
    "        detection_metrics = evaluate_model(\n",
    "            detection_model, \n",
    "            detection_test_loader, \n",
    "            detection_criterion, \n",
    "            device, \n",
    "            detection_class_names,\n",
    "            save_dir=MODELS_DIR,\n",
    "            model_name=\"detection\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'évaluation du modèle de détection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer le modèle de classification\n",
    "if data_available and 'classification_test_loader' in locals() and 'classification_class_names' in locals():\n",
    "    try:\n",
    "        print(\"=== Évaluation du modèle de classification ===\\n\")\n",
    "        \n",
    "        # Charger le meilleur modèle (selon l'accuracy)\n",
    "        classification_model.load_state_dict(torch.load(os.path.join(MODELS_DIR, 'classification_best_acc.pth')))\n",
    "        classification_model = classification_model.to(device)\n",
    "        \n",
    "        # Évaluer le modèle\n",
    "        classification_criterion = nn.CrossEntropyLoss()\n",
    "        classification_metrics = evaluate_model(\n",
    "            classification_model, \n",
    "            classification_test_loader, \n",
    "            classification_criterion, \n",
    "            device, \n",
    "            classification_class_names,\n",
    "            save_dir=MODELS_DIR,\n",
    "            model_name=\"classification\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'évaluation du modèle de classification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sauvegarde du modèle à deux étapes\n",
    "\n",
    "Créons et sauvegardons le modèle à deux étapes complet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from models.model_architecture import TwoStageModel\n",
    "\n",
    "# Créer et sauvegarder le modèle à deux étapes\n",
    "if data_available and 'detection_class_names' in locals() and 'classification_class_names' in locals():\n",
    "    try:\n",
    "        print(\"=== Création du modèle à deux étapes ===\\n\")\n",
    "        \n",
    "        # Charger les meilleurs modèles\n",
    "        detection_model.load_state_dict(torch.load(os.path.join(MODELS_DIR, 'detection_best_acc.pth')))\n",
    "        classification_model.load_state_dict(torch.load(os.path.join(MODELS_DIR, 'classification_best_acc.pth')))\n",
    "        \n",
    "        # Créer le modèle à deux étapes\n",
    "        two_stage_model = TwoStageModel(detection_model, classification_model, detection_threshold=0.7)\n",
    "        \n",
    "        # Sauvegarder les informations du modèle\n",
    "        model_info = {\n",
    "            'detection_classes': detection_class_names,\n",
    "            'classification_classes': classification_class_names,\n",
    "            'model_info': two_stage_model.get_model_info(),\n",
    "            'img_size': IMG_SIZE,\n",
    "            'date_created': time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(MODELS_DIR, 'two_stage_model_info.json'), 'w') as f:\n",
    "            json.dump(model_info, f, indent=4)\n",
    "        \n",
    "        print(\"Modèle à deux étapes créé et informations sauvegardées.\")\n",
    "        print(f\"Classes de détection: {detection_class_names}\")\n",
    "        print(f\"Classes de classification: {classification_class_names}\")\n",
    "        print(f\"Informations du modèle: {two_stage_model.get_model_info()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la création du modèle à deux étapes: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "Dans ce notebook, nous avons utilisé les fonctions existantes des modules `data_preprocessing`, `model_architecture` et `train_two_stage` pour :\n",
    "1. Charger les données\n",
    "2. Créer les modèles de détection et de classification\n",
    "3. Entraîner les modèles\n",
    "4. Évaluer les performances\n",
    "5. Créer et sauvegarder le modèle à deux étapes complet\n",
    "\n",
    "Le modèle est maintenant prêt à être testé sur de nouvelles images dans le notebook suivant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
