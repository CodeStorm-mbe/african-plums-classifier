{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test du modèle de classification des prunes africaines\n",
    "\n",
    "Ce notebook utilise les fonctions existantes dans le dépôt pour tester le modèle de classification des prunes africaines en utilisant le jeu de données Kaggle \"African Plums Dataset\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration de l'environnement pour Google Colab\n",
    "\n",
    "Commençons par cloner le dépôt GitHub et configurer l'environnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier si nous sommes dans Google Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"Exécution dans Google Colab: {IN_COLAB}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Cloner le dépôt GitHub\n",
    "    !git clone https://github.com/CodeStorm-mbe/african-plums-classifier.git\n",
    "    %cd african-plums-classifier\n",
    "    \n",
    "    # Installer les dépendances requises\n",
    "    !pip install -r requirements.txt\n",
    "    !pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Monter Google Drive pour la persistance des données\n",
    "\n",
    "Pour accéder aux modèles entraînés dans les notebooks précédents, nous allons utiliser Google Drive comme stockage persistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monter Google Drive si nous sommes dans Colab\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Définir les chemins dans Google Drive\n",
    "    DRIVE_PROJECT_DIR = \"/content/drive/MyDrive/african-plums-classifier\"\n",
    "    DRIVE_DATA_DIR = f\"{DRIVE_PROJECT_DIR}/data\"\n",
    "    DRIVE_KAGGLE_DIR = f\"{DRIVE_DATA_DIR}/kaggle\"\n",
    "    DRIVE_RAW_DATA_DIR = f\"{DRIVE_DATA_DIR}/raw\"\n",
    "    DRIVE_PLUM_DATA_DIR = f\"{DRIVE_RAW_DATA_DIR}/plums\"\n",
    "    DRIVE_NON_PLUM_DATA_DIR = f\"{DRIVE_RAW_DATA_DIR}/non_plums\"\n",
    "    DRIVE_MODELS_DIR = f\"{DRIVE_PROJECT_DIR}/models\"\n",
    "    DRIVE_TEST_IMAGES_DIR = f\"{DRIVE_PROJECT_DIR}/test_images\"\n",
    "    \n",
    "    # Vérifier si les répertoires existent, sinon les créer\n",
    "    !mkdir -p {DRIVE_PROJECT_DIR}\n",
    "    !mkdir -p {DRIVE_DATA_DIR}\n",
    "    !mkdir -p {DRIVE_KAGGLE_DIR}\n",
    "    !mkdir -p {DRIVE_RAW_DATA_DIR}\n",
    "    !mkdir -p {DRIVE_PLUM_DATA_DIR}\n",
    "    !mkdir -p {DRIVE_NON_PLUM_DATA_DIR}\n",
    "    !mkdir -p {DRIVE_MODELS_DIR}\n",
    "    !mkdir -p {DRIVE_TEST_IMAGES_DIR}\n",
    "    \n",
    "    print(f\"Google Drive monté et répertoires créés dans {DRIVE_PROJECT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import glob\n",
    "import random\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Ajouter le répertoire courant au chemin pour pouvoir importer nos modules\n",
    "if IN_COLAB:\n",
    "    # Dans Colab, nous sommes déjà dans le répertoire du projet\n",
    "    if \"/content/african-plums-classifier\" not in sys.path:\n",
    "        sys.path.append(\"/content/african-plums-classifier\")\n",
    "else:\n",
    "    # En local, ajouter le répertoire parent\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "# Importer nos modules personnalisés\n",
    "from data.data_preprocessing import preprocess_single_image\n",
    "from models.model_architecture import get_model, TwoStageModel\n",
    "\n",
    "# Définir les chemins\n",
    "if IN_COLAB:\n",
    "    # Utiliser les chemins dans Google Drive pour la persistance\n",
    "    DATA_ROOT = DRIVE_RAW_DATA_DIR\n",
    "    KAGGLE_DIR = DRIVE_KAGGLE_DIR\n",
    "    MODELS_DIR = DRIVE_MODELS_DIR\n",
    "    TEST_IMAGES_DIR = DRIVE_TEST_IMAGES_DIR\n",
    "    \n",
    "    # Créer également des liens symboliques pour faciliter l'accès depuis le code existant\n",
    "    LOCAL_DATA_ROOT = \"data/raw\"\n",
    "    LOCAL_KAGGLE_DIR = \"data/kaggle\"\n",
    "    LOCAL_MODELS_DIR = \"models/saved\"\n",
    "    LOCAL_TEST_IMAGES_DIR = \"data/test_images\"\n",
    "    \n",
    "    # Créer les répertoires locaux s'ils n'existent pas\n",
    "    !mkdir -p {LOCAL_DATA_ROOT}\n",
    "    !mkdir -p {LOCAL_KAGGLE_DIR}\n",
    "    !mkdir -p {LOCAL_MODELS_DIR}\n",
    "    !mkdir -p {LOCAL_TEST_IMAGES_DIR}\n",
    "    \n",
    "    # Créer des liens symboliques si nécessaire\n",
    "    if not os.path.exists(LOCAL_DATA_ROOT) or not os.path.islink(LOCAL_DATA_ROOT):\n",
    "        !rm -rf {LOCAL_DATA_ROOT}\n",
    "        !ln -s {DATA_ROOT} {LOCAL_DATA_ROOT}\n",
    "    \n",
    "    if not os.path.exists(LOCAL_KAGGLE_DIR) or not os.path.islink(LOCAL_KAGGLE_DIR):\n",
    "        !rm -rf {LOCAL_KAGGLE_DIR}\n",
    "        !ln -s {KAGGLE_DIR} {LOCAL_KAGGLE_DIR}\n",
    "        \n",
    "    if not os.path.exists(LOCAL_MODELS_DIR) or not os.path.islink(LOCAL_MODELS_DIR):\n",
    "        !rm -rf {LOCAL_MODELS_DIR}\n",
    "        !ln -s {MODELS_DIR} {LOCAL_MODELS_DIR}\n",
    "        \n",
    "    if not os.path.exists(LOCAL_TEST_IMAGES_DIR) or not os.path.islink(LOCAL_TEST_IMAGES_DIR):\n",
    "        !rm -rf {LOCAL_TEST_IMAGES_DIR}\n",
    "        !ln -s {TEST_IMAGES_DIR} {LOCAL_TEST_IMAGES_DIR}\n",
    "else:\n",
    "    # En local\n",
    "    DATA_ROOT = \"../data/raw\"\n",
    "    KAGGLE_DIR = \"../data/kaggle\"\n",
    "    MODELS_DIR = \"../models/saved\"\n",
    "    TEST_IMAGES_DIR = \"../data/test_images\"\n",
    "\n",
    "PLUM_DATA_DIR = os.path.join(DATA_ROOT, \"plums\")  # Sous-dossier pour les prunes\n",
    "NON_PLUM_DATA_DIR = os.path.join(DATA_ROOT, \"non_plums\")  # Sous-dossier pour les non-prunes\n",
    "\n",
    "# Créer les répertoires s'ils n'existent pas\n",
    "os.makedirs(PLUM_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(NON_PLUM_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(TEST_IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(KAGGLE_DIR, exist_ok=True)\n",
    "\n",
    "# Définir les paramètres\n",
    "IMG_SIZE = 224\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Fixer les seeds pour la reproductibilité\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Déterminer le device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilisation de: {device}\")\n",
    "\n",
    "# Afficher les chemins des données\n",
    "print(f\"\\nChemins des données:\")\n",
    "print(f\"DATA_ROOT: {DATA_ROOT}\")\n",
    "print(f\"KAGGLE_DIR: {KAGGLE_DIR}\")\n",
    "print(f\"PLUM_DATA_DIR: {PLUM_DATA_DIR}\")\n",
    "print(f\"NON_PLUM_DATA_DIR: {NON_PLUM_DATA_DIR}\")\n",
    "print(f\"MODELS_DIR: {MODELS_DIR}\")\n",
    "print(f\"TEST_IMAGES_DIR: {TEST_IMAGES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration de l'API Kaggle\n",
    "\n",
    "Pour télécharger le jeu de données Kaggle, nous devons configurer l'API Kaggle si ce n'est pas déjà fait dans les notebooks précédents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de l'API Kaggle\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    \n",
    "    # Vérifier si le fichier kaggle.json existe dans Google Drive\n",
    "    KAGGLE_CONFIG_PATH = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "    DRIVE_KAGGLE_CONFIG_PATH = f\"{DRIVE_PROJECT_DIR}/kaggle.json\"\n",
    "    \n",
    "    # Vérifier si le fichier kaggle.json existe dans Google Drive\n",
    "    kaggle_config_in_drive = os.path.exists(DRIVE_KAGGLE_CONFIG_PATH)\n",
    "    \n",
    "    # Vérifier si le fichier kaggle.json existe localement\n",
    "    kaggle_config_exists = os.path.exists(KAGGLE_CONFIG_PATH)\n",
    "    \n",
    "    if kaggle_config_in_drive:\n",
    "        print(f\"Fichier kaggle.json trouvé dans Google Drive. Utilisation de ce fichier.\")\n",
    "        # Créer le répertoire .kaggle s'il n'existe pas\n",
    "        os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n",
    "        # Copier le fichier de Google Drive vers le répertoire local\n",
    "        shutil.copy(DRIVE_KAGGLE_CONFIG_PATH, KAGGLE_CONFIG_PATH)\n",
    "        # Définir les permissions appropriées\n",
    "        os.chmod(KAGGLE_CONFIG_PATH, 600)\n",
    "        print(\"Fichier kaggle.json configuré avec succès.\")\n",
    "    elif not kaggle_config_exists:\n",
    "        print(\"Veuillez télécharger votre fichier kaggle.json pour l'authentification Kaggle.\")\n",
    "        print(\"Vous pouvez le générer sur https://www.kaggle.com/account dans la section 'API'.\")\n",
    "        \n",
    "        # Télécharger le fichier kaggle.json\n",
    "        uploaded = files.upload()\n",
    "        \n",
    "        # Créer le répertoire .kaggle s'il n'existe pas\n",
    "        os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n",
    "        \n",
    "        # Déplacer le fichier kaggle.json vers le répertoire .kaggle\n",
    "        if 'kaggle.json' in uploaded:\n",
    "            shutil.move('kaggle.json', KAGGLE_CONFIG_PATH)\n",
    "            # Définir les permissions appropriées\n",
    "            os.chmod(KAGGLE_CONFIG_PATH, 600)\n",
    "            # Sauvegarder également dans Google Drive pour une utilisation future\n",
    "            shutil.copy(KAGGLE_CONFIG_PATH, DRIVE_KAGGLE_CONFIG_PATH)\n",
    "            print(\"Fichier kaggle.json configuré avec succès et sauvegardé dans Google Drive.\")\n",
    "        else:\n",
    "            print(\"Erreur: Le fichier kaggle.json n'a pas été téléchargé.\")\n",
    "    else:\n",
    "        print(\"Le fichier kaggle.json existe déjà localement.\")\n",
    "        # Sauvegarder également dans Google Drive pour une utilisation future\n",
    "        shutil.copy(KAGGLE_CONFIG_PATH, DRIVE_KAGGLE_CONFIG_PATH)\n",
    "        print(\"Fichier kaggle.json sauvegardé dans Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vérification des modèles entraînés dans Google Drive\n",
    "\n",
    "Vérifions si les modèles ont déjà été entraînés dans les notebooks précédents et sont disponibles dans Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier si les modèles ont été entraînés dans les notebooks précédents\n",
    "def check_models_availability():\n",
    "    \"\"\"Vérifie si les modèles ont été entraînés dans les notebooks précédents.\"\"\"\n",
    "    if IN_COLAB:\n",
    "        # Vérifier si le fichier d'informations d'entraînement existe dans Google Drive\n",
    "        training_info_path = f\"{DRIVE_PROJECT_DIR}/training_info.json\"\n",
    "        if os.path.exists(training_info_path):\n",
    "            try:\n",
    "                with open(training_info_path, 'r') as f:\n",
    "                    training_info = json.load(f)\n",
    "                \n",
    "                print(f\"Informations d'entraînement trouvées dans Google Drive.\")\n",
    "                print(f\"Date d'entraînement: {training_info.get('date_trained', 'date inconnue')}\")\n",
    "                return training_info\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors de la lecture du fichier d'informations d'entraînement: {e}\")\n",
    "    \n",
    "    # Vérifier si les fichiers de modèle existent\n",
    "    detection_model_path = os.path.join(MODELS_DIR, 'detection_best_acc.pth')\n",
    "    classification_model_path = os.path.join(MODELS_DIR, 'classification_best_acc.pth')\n",
    "    model_info_path = os.path.join(MODELS_DIR, 'two_stage_model_info.json')\n",
    "    \n",
    "    if os.path.exists(detection_model_path) and os.path.exists(classification_model_path) and os.path.exists(model_info_path):\n",
    "        print(\"Les fichiers de modèle existent.\")\n",
    "        \n",
    "        # Lire les informations du modèle\n",
    "        try:\n",
    "            with open(model_info_path, 'r') as f:\n",
    "                model_info = json.load(f)\n",
    "            \n",
    "            print(f\"Date de création du modèle: {model_info.get('date_created', 'date inconnue')}\")\n",
    "            return {\n",
    "                'model_info': model_info,\n",
    "                'models_available': True\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la lecture du fichier d'informations du modèle: {e}\")\n",
    "    else:\n",
    "        print(\"Les fichiers de modèle n'existent pas.\")\n",
    "        if not os.path.exists(detection_model_path):\n",
    "            print(f\"  - Fichier manquant: {detection_model_path}\")\n",
    "        if not os.path.exists(classification_model_path):\n",
    "            print(f\"  - Fichier manquant: {classification_model_path}\")\n",
    "        if not os.path.exists(model_info_path):\n",
    "            print(f\"  - Fichier manquant: {model_info_path}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Vérifier si les modèles ont été entraînés\n",
    "training_info = check_models_availability()\n",
    "\n",
    "if training_info:\n",
    "    print(\"\\nInformations sur les modèles entraînés:\")\n",
    "    if 'model_info' in training_info:\n",
    "        model_info = training_info['model_info']\n",
    "        print(f\"Classes de détection: {model_info.get('detection_classes', [])}\")\n",
    "        print(f\"Classes de classification: {model_info.get('classification_classes', [])}\")\n",
    "else:\n",
    "    print(\"\\nVeuillez d'abord exécuter le notebook d'entraînement des modèles.\")\n",
    "    \n",
    "    # Si nous sommes dans Colab, proposer de télécharger les modèles\n",
    "    if IN_COLAB:\n",
    "        print(\"\\nVous pouvez télécharger les modèles entraînés ci-dessous.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Téléchargement des modèles entraînés (pour Google Colab)\n",
    "\n",
    "Si vous êtes dans Google Colab et que les modèles ne sont pas disponibles dans Google Drive, vous pouvez les télécharger ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB and (training_info is None):\n",
    "    # Créer une fonction pour télécharger les modèles\n",
    "    def upload_models():\n",
    "        print(\"Veuillez télécharger les fichiers de modèle suivants:\")\n",
    "        print(\"1. detection_best_acc.pth\")\n",
    "        print(\"2. classification_best_acc.pth\")\n",
    "        print(\"3. two_stage_model_info.json\")\n",
    "        \n",
    "        # Télécharger les fichiers\n",
    "        uploaded = files.upload()\n",
    "        \n",
    "        # Déplacer les fichiers vers le répertoire des modèles\n",
    "        for filename in uploaded.keys():\n",
    "            dst_path = os.path.join(MODELS_DIR, filename)\n",
    "            shutil.move(filename, dst_path)\n",
    "            print(f\"Fichier {filename} déplacé vers {dst_path}\")\n",
    "    \n",
    "    # Demander à l'utilisateur s'il souhaite télécharger les modèles\n",
    "    print(\"Souhaitez-vous télécharger les modèles maintenant? (Décommentez la ligne suivante pour télécharger)\")\n",
    "    # upload_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Préparation des images de test à partir du jeu de données Kaggle\n",
    "\n",
    "Téléchargeons et préparons des images de test à partir du jeu de données Kaggle \"African Plums Dataset\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_kaggle_dataset(force_download=False):\n",
    "    \"\"\"Télécharge le jeu de données Kaggle 'African Plums Dataset'.\"\"\"\n",
    "    # Vérifier si le jeu de données a déjà été téléchargé\n",
    "    dataset_zip = os.path.join(KAGGLE_DIR, 'african-plums-dataset.zip')\n",
    "    if os.path.exists(dataset_zip) and not force_download:\n",
    "        print(f\"Le jeu de données a déjà été téléchargé dans {dataset_zip}.\")\n",
    "        return dataset_zip\n",
    "    \n",
    "    print(\"Téléchargement du jeu de données Kaggle 'African Plums Dataset'...\")\n",
    "    try:\n",
    "        # Télécharger le jeu de données\n",
    "        !kaggle datasets download -d arnaudfadja/african-plums-quality-and-defect-assessment-data -p {KAGGLE_DIR}\n",
    "        print(f\"Jeu de données téléchargé avec succès dans {dataset_zip}.\")\n",
    "        return dataset_zip\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du téléchargement du jeu de données: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_and_prepare_test_images(dataset_zip, force_extract=False, num_images_per_class=3):\n",
    "    \"\"\"Extrait et prépare des images de test à partir du jeu de données Kaggle.\"\"\"\n",
    "    if not os.path.exists(dataset_zip):\n",
    "        print(f\"Le fichier {dataset_zip} n'existe pas.\")\n",
    "        return False\n",
    "    \n",
    "    # Vérifier si les données ont déjà été extraites\n",
    "    extracted_dir = os.path.join(KAGGLE_DIR, 'extracted')\n",
    "    if os.path.exists(extracted_dir) and not force_extract:\n",
    "        print(f\"Le jeu de données a déjà été extrait dans {extracted_dir}.\")\n",
    "    else:\n",
    "        print(f\"Extraction du jeu de données...\")\n",
    "        os.makedirs(extracted_dir, exist_ok=True)\n",
    "        \n",
    "        # Extraire le fichier zip\n",
    "        with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extracted_dir)\n",
    "        \n",
    "        print(f\"Jeu de données extrait avec succès dans {extracted_dir}.\")\n",
    "    \n",
    "    # Préparer des images de test\n",
    "    print(\"Préparation des images de test...\")\n",
    "    \n",
    "    # Vérifier la structure du jeu de données extrait\n",
    "    dataset_dir = os.path.join(extracted_dir, 'african_plums_dataset')\n",
    "    if not os.path.exists(dataset_dir):\n",
    "        print(f\"Le répertoire {dataset_dir} n'existe pas.\")\n",
    "        return False\n",
    "    \n",
    "    # Mapper les classes du jeu de données Kaggle\n",
    "    classes = ['bruised', 'cracked', 'rotten', 'spotted', 'unaffected', 'unripe']\n",
    "    \n",
    "    # Sélectionner des images aléatoires pour chaque classe\n",
    "    for cls in classes:\n",
    "        src_dir = os.path.join(dataset_dir, cls)\n",
    "        if not os.path.exists(src_dir):\n",
    "            print(f\"Le répertoire {src_dir} n'existe pas.\")\n",
    "            continue\n",
    "        \n",
    "        # Obtenir la liste des images\n",
    "        images = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        if not images:\n",
    "            print(f\"Aucune image trouvée dans {src_dir}.\")\n",
    "            continue\n",
    "        \n",
    "        # Sélectionner des images aléatoires\n",
    "        selected_images = random.sample(images, min(num_images_per_class, len(images)))\n",
    "        \n",
    "        # Copier les images sélectionnées vers le répertoire de test\n",
    "        for i, img_name in enumerate(selected_images):\n",
    "            src_path = os.path.join(src_dir, img_name)\n",
    "            dst_path = os.path.join(TEST_IMAGES_DIR, f\"test_{cls}_{i+1}.jpg\")\n",
    "            shutil.copy(src_path, dst_path)\n",
    "            print(f\"Image copiée: {src_path} -> {dst_path}\")\n",
    "    \n",
    "    # Ajouter des images non-prune (si disponibles)\n",
    "    non_plum_dir = os.path.join(NON_PLUM_DATA_DIR, \"non_plum\")\n",
    "    if os.path.exists(non_plum_dir):\n",
    "        non_plum_images = [f for f in os.listdir(non_plum_dir) if os.path.isfile(os.path.join(non_plum_dir, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        if non_plum_images:\n",
    "            selected_non_plum = random.sample(non_plum_images, min(5, len(non_plum_images)))\n",
    "            for i, img_name in enumerate(selected_non_plum):\n",
    "                src_path = os.path.join(non_plum_dir, img_name)\n",
    "                dst_path = os.path.join(TEST_IMAGES_DIR, f\"test_non_plum_{i+1}.jpg\")\n",
    "                shutil.copy(src_path, dst_path)\n",
    "                print(f\"Image non-prune copiée: {src_path} -> {dst_path}\")\n",
    "    \n",
    "    # Vérifier le nombre d'images de test préparées\n",
    "    test_images = [f for f in os.listdir(TEST_IMAGES_DIR) if os.path.isfile(os.path.join(TEST_IMAGES_DIR, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    print(f\"\\n{len(test_images)} images de test préparées avec succès.\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def prepare_test_images(force_prepare=False, num_images_per_class=3):\n",
    "    \"\"\"Prépare des images de test à partir du jeu de données Kaggle ou crée des images synthétiques.\"\"\"\n",
    "    # Vérifier si des images de test existent déjà\n",
    "    existing_images = [f for f in os.listdir(TEST_IMAGES_DIR) if os.path.isfile(os.path.join(TEST_IMAGES_DIR, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    if existing_images and not force_prepare:\n",
    "        print(f\"Des images de test existent déjà ({len(existing_images)} images). Utilisez force_prepare=True pour les remplacer.\")\n",
    "        return\n",
    "    \n",
    "    # Essayer de télécharger et préparer des images de test à partir du jeu de données Kaggle\n",
    "    try:\n",
    "        dataset_zip = download_kaggle_dataset(force_download=False)\n",
    "        if dataset_zip:\n",
    "            success = extract_and_prepare_test_images(dataset_zip, force_extract=False, num_images_per_class=num_images_per_class)\n",
    "            if success:\n",
    "                print(\"Images de test préparées avec succès à partir du jeu de données Kaggle.\")\n",
    "                return\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la préparation des images de test à partir du jeu de données Kaggle: {e}\")\n",
    "    \n",
    "    # Si le téléchargement ou la préparation échoue, créer des images synthétiques\n",
    "    print(\"Création d'images de test synthétiques...\")\n",
    "    \n",
    "    # Définir les couleurs pour les différentes classes\n",
    "    colors = {\n",
    "        \"bruised\": (150, 0, 0),      # Rouge foncé\n",
    "        \"cracked\": (150, 100, 0),    # Marron\n",
    "        \"rotten\": (100, 0, 100),     # Violet\n",
    "        \"spotted\": (150, 50, 50),    # Rouge-rose\n",
    "        \"unaffected\": (150, 50, 0),  # Orange-rouge\n",
    "        \"unripe\": (0, 150, 0)        # Vert foncé\n",
    "    }\n",
    "    \n",
    "    # Créer des images pour chaque classe de prune\n",
    "    for cls, base_color in colors.items():\n",
    "        for i in range(3):  # 3 images par classe\n",
    "            # Ajouter un peu de variation aléatoire à la couleur\n",
    "            color_var = [max(0, min(255, c + random.randint(-20, 20))) for c in base_color]\n",
    "            \n",
    "            # Créer une image\n",
    "            img = Image.new('RGB', (224, 224), (255, 255, 255))\n",
    "            pixels = img.load()\n",
    "            \n",
    "            # Dessiner un cercle approximatif avec la couleur\n",
    "            center_x, center_y = 112, 112\n",
    "            radius = 100 + random.randint(-10, 10)\n",
    "            \n",
    "            for x in range(img.width):\n",
    "                for y in range(img.height):\n",
    "                    dist = ((x - center_x) ** 2 + (y - center_y) ** 2) ** 0.5\n",
    "                    if dist <= radius:\n",
    "                        # Ajouter du bruit à chaque pixel\n",
    "                        pixel_color = [max(0, min(255, c + random.randint(-10, 10))) for c in color_var]\n",
    "                        pixels[x, y] = tuple(pixel_color)\n",
    "            \n",
    "            # Sauvegarder l'image\n",
    "            img_path = os.path.join(TEST_IMAGES_DIR, f\"test_{cls}_{i+1}.jpg\")\n",
    "            img.save(img_path)\n",
    "    \n",
    "    # Créer des images non-prune\n",
    "    for i in range(5):  # 5 images non-prune\n",
    "        # Couleur aléatoire qui n'est pas proche des couleurs de prune\n",
    "        color = (random.randint(0, 100), random.randint(150, 255), random.randint(150, 255))\n",
    "        \n",
    "        # Créer une image\n",
    "        img = Image.new('RGB', (224, 224), (255, 255, 255))\n",
    "        pixels = img.load()\n",
    "        \n",
    "        # Dessiner une forme aléatoire (carré ou triangle)\n",
    "        shape = random.choice(['square', 'triangle'])\n",
    "        \n",
    "        if shape == 'square':\n",
    "            # Dessiner un carré\n",
    "            size = random.randint(100, 150)\n",
    "            top_left = (random.randint(0, 224-size), random.randint(0, 224-size))\n",
    "            \n",
    "            for x in range(top_left[0], top_left[0] + size):\n",
    "                for y in range(top_left[1], top_left[1] + size):\n",
    "                    if 0 <= x < 224 and 0 <= y < 224:\n",
    "                        # Ajouter du bruit à chaque pixel\n",
    "                        pixel_color = [max(0, min(255, c + random.randint(-10, 10))) for c in color]\n",
    "                        pixels[x, y] = tuple(pixel_color)\n",
    "        else:\n",
    "            # Dessiner un triangle\n",
    "            p1 = (random.randint(50, 174), random.randint(50, 174))\n",
    "            p2 = (p1[0] + random.randint(30, 50), p1[1] + random.randint(30, 50))\n",
    "            p3 = (p1[0] - random.randint(0, 30), p2[1])\n",
    "            \n",
    "            # Remplir le triangle (algorithme simple)\n",
    "            min_x = min(p1[0], p2[0], p3[0])\n",
    "            max_x = max(p1[0], p2[0], p3[0])\n",
    "            min_y = min(p1[1], p2[1], p3[1])\n",
    "            max_y = max(p1[1], p2[1], p3[1])\n",
    "            \n",
    "            for x in range(min_x, max_x + 1):\n",
    "                for y in range(min_y, max_y + 1):\n",
    "                    if 0 <= x < 224 and 0 <= y < 224:\n",
    "                        # Vérification simple si le point est dans le triangle\n",
    "                        if (x >= p1[0] and y >= p1[1] and x <= p2[0] and y <= p2[1]):\n",
    "                            # Ajouter du bruit à chaque pixel\n",
    "                            pixel_color = [max(0, min(255, c + random.randint(-10, 10))) for c in color]\n",
    "                            pixels[x, y] = tuple(pixel_color)\n",
    "        \n",
    "        # Sauvegarder l'image\n",
    "        img_path = os.path.join(TEST_IMAGES_DIR, f\"test_non_plum_{i+1}.jpg\")\n",
    "        img.save(img_path)\n",
    "    \n",
    "    # Vérifier le nombre d'images de test créées\n",
    "    test_images = [f for f in os.listdir(TEST_IMAGES_DIR) if os.path.isfile(os.path.join(TEST_IMAGES_DIR, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    print(f\"\\n{len(test_images)} images de test synthétiques créées avec succès.\")\n",
    "\n",
    "# Préparer des images de test\n",
    "prepare_test_images(force_prepare=False, num_images_per_class=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualisation des images de test\n",
    "\n",
    "Visualisons les images de test que nous avons préparées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_test_images():\n",
    "    \"\"\"Visualise les images de test.\"\"\"\n",
    "    # Obtenir la liste des images de test\n",
    "    test_images = [f for f in os.listdir(TEST_IMAGES_DIR) if os.path.isfile(os.path.join(TEST_IMAGES_DIR, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    if not test_images:\n",
    "        print(\"Aucune image de test trouvée.\")\n",
    "        return\n",
    "    \n",
    "    # Trier les images par classe\n",
    "    test_images.sort()\n",
    "    \n",
    "    # Déterminer le nombre de lignes et de colonnes pour l'affichage\n",
    "    num_images = min(len(test_images), 15)  # Limiter à 15 images pour l'affichage\n",
    "    num_cols = 5\n",
    "    num_rows = (num_images + num_cols - 1) // num_cols\n",
    "    \n",
    "    # Créer la figure\n",
    "    plt.figure(figsize=(15, 3 * num_rows))\n",
    "    \n",
    "    # Afficher chaque image\n",
    "    for i, img_name in enumerate(test_images[:num_images]):\n",
    "        img_path = os.path.join(TEST_IMAGES_DIR, img_name)\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        plt.subplot(num_rows, num_cols, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(img_name.replace('test_', '').replace('.jpg', ''))\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Sauvegarder la figure dans Google Drive si nous sommes dans Colab\n",
    "    if IN_COLAB:\n",
    "        # Sauvegarder la figure localement\n",
    "        plt.savefig('test_images.png')\n",
    "        # Copier la figure dans Google Drive\n",
    "        test_images_fig_path = f\"{DRIVE_PROJECT_DIR}/test_images.png\"\n",
    "        shutil.copy('test_images.png', test_images_fig_path)\n",
    "        print(f\"Figure des images de test sauvegardée dans Google Drive: {test_images_fig_path}\")\n",
    "\n",
    "# Visualiser les images de test\n",
    "visualize_test_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Chargement du modèle\n",
    "\n",
    "Chargeons le modèle à deux étapes que nous avons entraîné précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_two_stage_model():\n",
    "    \"\"\"Charge le modèle à deux étapes à partir des fichiers sauvegardés.\"\"\"\n",
    "    # Vérifier si les fichiers nécessaires existent\n",
    "    model_info_path = os.path.join(MODELS_DIR, 'two_stage_model_info.json')\n",
    "    detection_model_path = os.path.join(MODELS_DIR, 'detection_best_acc.pth')\n",
    "    classification_model_path = os.path.join(MODELS_DIR, 'classification_best_acc.pth')\n",
    "    \n",
    "    if not os.path.exists(model_info_path):\n",
    "        raise FileNotFoundError(f\"Le fichier d'informations du modèle n'existe pas: {model_info_path}\")\n",
    "    if not os.path.exists(detection_model_path):\n",
    "        raise FileNotFoundError(f\"Le fichier du modèle de détection n'existe pas: {detection_model_path}\")\n",
    "    if not os.path.exists(classification_model_path):\n",
    "        raise FileNotFoundError(f\"Le fichier du modèle de classification n'existe pas: {classification_model_path}\")\n",
    "    \n",
    "    # Charger les informations du modèle\n",
    "    with open(model_info_path, 'r') as f:\n",
    "        model_info = json.load(f)\n",
    "    \n",
    "    # Extraire les informations\n",
    "    detection_classes = model_info['detection_classes']\n",
    "    classification_classes = model_info['classification_classes']\n",
    "    model_architecture_info = model_info['model_info']\n",
    "    \n",
    "    # Créer les modèles\n",
    "    detection_model = get_model(\n",
    "        model_name=model_architecture_info['detection_model']['base_model'].split('_')[0] if '_' in model_architecture_info['detection_model']['base_model'] else 'standard',\n",
    "        num_classes=len(detection_classes),\n",
    "        base_model=model_architecture_info['detection_model']['base_model'].split('_')[1] if '_' in model_architecture_info['detection_model']['base_model'] else model_architecture_info['detection_model']['base_model'],\n",
    "        pretrained=False\n",
    "    )\n",
    "    \n",
    "    classification_model = get_model(\n",
    "        model_name=model_architecture_info['classification_model']['base_model'].split('_')[0] if '_' in model_architecture_info['classification_model']['base_model'] else 'standard',\n",
    "        num_classes=len(classification_classes),\n",
    "        base_model=model_architecture_info['classification_model']['base_model'].split('_')[1] if '_' in model_architecture_info['classification_model']['base_model'] else model_architecture_info['classification_model']['base_model'],\n",
    "        pretrained=False\n",
    "    )\n",
    "    \n",
    "    # Charger les poids\n",
    "    detection_model.load_state_dict(torch.load(detection_model_path, map_location=device))\n",
    "    classification_model.load_state_dict(torch.load(classification_model_path, map_location=device))\n",
    "    \n",
    "    # Créer le modèle à deux étapes\n",
    "    two_stage_model = TwoStageModel(\n",
    "        detection_model, \n",
    "        classification_model, \n",
    "        detection_threshold=model_architecture_info['detection_threshold'] if 'detection_threshold' in model_architecture_info else 0.7\n",
    "    )\n",
    "    \n",
    "    return two_stage_model, detection_classes, classification_classes\n",
    "\n",
    "# Charger le modèle\n",
    "try:\n",
    "    model, detection_classes, classification_classes = load_two_stage_model()\n",
    "    model.detection_model.to(device)\n",
    "    model.classification_model.to(device)\n",
    "    \n",
    "    print(\"Modèle à deux étapes chargé avec succès!\")\n",
    "    print(f\"Classes de détection: {detection_classes}\")\n",
    "    print(f\"Classes de classification: {classification_classes}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement du modèle: {e}\")\n",
    "    print(\"\\nSi vous n'avez pas encore entraîné le modèle, veuillez d'abord exécuter le notebook d'entraînement.\")\n",
    "    \n",
    "    # Créer des classes fictives pour pouvoir continuer le notebook\n",
    "    detection_classes = ['plum', 'non_plum']\n",
    "    classification_classes = ['bruised', 'cracked', 'rotten', 'spotted', 'unaffected', 'unripe']\n",
    "    print(\"\\nUtilisation de classes fictives pour la démonstration:\")\n",
    "    print(f\"Classes de détection: {detection_classes}\")\n",
    "    print(f\"Classes de classification: {classification_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prédiction sur une image individuelle\n",
    "\n",
    "Testons notre modèle sur une image individuelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_image(model, img_path, detection_classes, classification_classes):\n",
    "    \"\"\"Prédit la classe d'une image individuelle.\"\"\"\n",
    "    # Vérifier si le fichier existe\n",
    "    if not os.path.exists(img_path):\n",
    "        print(f\"Le fichier {img_path} n'existe pas.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Charger et prétraiter l'image\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_tensor = preprocess_single_image(img, IMG_SIZE)\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(device)  # Ajouter la dimension du batch\n",
    "    \n",
    "    # Passer en mode évaluation\n",
    "    model.detection_model.eval()\n",
    "    model.classification_model.eval()\n",
    "    \n",
    "    # Prédire\n",
    "    with torch.no_grad():\n",
    "        # Prédiction de détection\n",
    "        detection_output = model.detection_model(img_tensor)\n",
    "        detection_probs = torch.nn.functional.softmax(detection_output, dim=1)\n",
    "        detection_pred = torch.argmax(detection_probs, dim=1).item()\n",
    "        detection_prob = detection_probs[0, detection_pred].item()\n",
    "        \n",
    "        # Si c'est une prune, prédire la classe\n",
    "        if detection_classes[detection_pred] == 'plum' and detection_prob >= model.detection_threshold:\n",
    "            # Prédiction de classification\n",
    "            classification_output = model.classification_model(img_tensor)\n",
    "            classification_probs = torch.nn.functional.softmax(classification_output, dim=1)\n",
    "            classification_pred = torch.argmax(classification_probs, dim=1).item()\n",
    "            classification_prob = classification_probs[0, classification_pred].item()\n",
    "            \n",
    "            return detection_classes[detection_pred], classification_classes[classification_pred], {\n",
    "                'detection_prob': detection_prob,\n",
    "                'classification_prob': classification_prob,\n",
    "                'detection_probs': detection_probs.cpu().numpy()[0],\n",
    "                'classification_probs': classification_probs.cpu().numpy()[0]\n",
    "            }\n",
    "        else:\n",
    "            return detection_classes[detection_pred], None, {\n",
    "                'detection_prob': detection_prob,\n",
    "                'detection_probs': detection_probs.cpu().numpy()[0]\n",
    "            }\n",
    "\n",
    "def visualize_prediction(img_path, detection_pred, classification_pred, probs):\n",
    "    \"\"\"Visualise l'image avec sa prédiction.\"\"\"\n",
    "    # Charger l'image\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    # Créer la figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Afficher l'image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Image de test\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Afficher les prédictions\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Texte de prédiction\n",
    "    text = f\"Détection: {detection_pred} ({probs['detection_prob']:.2f})\\n\"\n",
    "    if classification_pred is not None:\n",
    "        text += f\"Classification: {classification_pred} ({probs['classification_prob']:.2f})\"\n",
    "    else:\n",
    "        text += \"Classification: N/A\"\n",
    "    \n",
    "    plt.text(0.1, 0.5, text, fontsize=12)\n",
    "    \n",
    "    # Afficher les probabilités de détection\n",
    "    if 'detection_probs' in probs:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.bar(detection_classes, probs['detection_probs'])\n",
    "        plt.title(\"Probabilités de détection\")\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Afficher les probabilités de classification si disponibles\n",
    "        if 'classification_probs' in probs:\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.bar(classification_classes, probs['classification_probs'])\n",
    "            plt.title(\"Probabilités de classification\")\n",
    "            plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Sauvegarder les figures dans Google Drive si nous sommes dans Colab\n",
    "    if IN_COLAB:\n",
    "        # Sauvegarder la figure localement\n",
    "        plt.savefig('prediction_result.png')\n",
    "        # Copier la figure dans Google Drive\n",
    "        prediction_fig_path = f\"{DRIVE_PROJECT_DIR}/prediction_result.png\"\n",
    "        shutil.copy('prediction_result.png', prediction_fig_path)\n",
    "        print(f\"Figure de prédiction sauvegardée dans Google Drive: {prediction_fig_path}\")\n",
    "\n",
    "# Tester le modèle sur une image individuelle\n",
    "if 'model' in locals():\n",
    "    # Obtenir la liste des images de test\n",
    "    test_images = [f for f in os.listdir(TEST_IMAGES_DIR) if os.path.isfile(os.path.join(TEST_IMAGES_DIR, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    if test_images:\n",
    "        # Sélectionner une image aléatoire\n",
    "        test_img = random.choice(test_images)\n",
    "        test_img_path = os.path.join(TEST_IMAGES_DIR, test_img)\n",
    "        \n",
    "        print(f\"Test du modèle sur l'image: {test_img}\")\n",
    "        detection_pred, classification_pred, probs = predict_single_image(model, test_img_path, detection_classes, classification_classes)\n",
    "        \n",
    "        if detection_pred is not None:\n",
    "            print(f\"Prédiction de détection: {detection_pred} (probabilité: {probs['detection_prob']:.2f})\")\n",
    "            if classification_pred is not None:\n",
    "                print(f\"Prédiction de classification: {classification_pred} (probabilité: {probs['classification_prob']:.2f})\")\n",
    "            else:\n",
    "                print(\"Prédiction de classification: N/A\")\n",
    "            \n",
    "            # Visualiser la prédiction\n",
    "            visualize_prediction(test_img_path, detection_pred, classification_pred, probs)\n",
    "        else:\n",
    "            print(\"Erreur lors de la prédiction.\")\n",
    "    else:\n",
    "        print(\"Aucune image de test trouvée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Prédiction sur toutes les images de test\n",
    "\n",
    "Testons notre modèle sur toutes les images de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all_test_images(model, test_dir, detection_classes, classification_classes):\n",
    "    \"\"\"Prédit la classe de toutes les images de test.\"\"\"\n",
    "    # Obtenir la liste des images de test\n",
    "    test_images = [f for f in os.listdir(test_dir) if os.path.isfile(os.path.join(test_dir, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    if not test_images:\n",
    "        print(\"Aucune image de test trouvée.\")\n",
    "        return None\n",
    "    \n",
    "    # Prédire pour chaque image\n",
    "    results = []\n",
    "    for img_name in test_images:\n",
    "        img_path = os.path.join(test_dir, img_name)\n",
    "        \n",
    "        # Extraire la vraie classe à partir du nom de fichier\n",
    "        true_class = None\n",
    "        for cls in classification_classes:\n",
    "            if cls in img_name:\n",
    "                true_class = cls\n",
    "                break\n",
    "        \n",
    "        if 'non_plum' in img_name:\n",
    "            true_detection = 'non_plum'\n",
    "            true_class = None\n",
    "        else:\n",
    "            true_detection = 'plum'\n",
    "        \n",
    "        # Prédire\n",
    "        detection_pred, classification_pred, probs = predict_single_image(model, img_path, detection_classes, classification_classes)\n",
    "        \n",
    "        # Ajouter aux résultats\n",
    "        results.append({\n",
    "            'img_name': img_name,\n",
    "            'img_path': img_path,\n",
    "            'true_detection': true_detection,\n",
    "            'true_class': true_class,\n",
    "            'detection_pred': detection_pred,\n",
    "            'classification_pred': classification_pred,\n",
    "            'probs': probs\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_test_results(results):\n",
    "    \"\"\"Visualise les résultats des prédictions sur les images de test.\"\"\"\n",
    "    if not results:\n",
    "        print(\"Aucun résultat à visualiser.\")\n",
    "        return\n",
    "    \n",
    "    # Calculer les métriques de détection\n",
    "    detection_correct = sum(1 for r in results if r['true_detection'] == r['detection_pred'])\n",
    "    detection_accuracy = detection_correct / len(results)\n",
    "    \n",
    "    # Calculer les métriques de classification\n",
    "    classification_results = [r for r in results if r['true_detection'] == 'plum' and r['detection_pred'] == 'plum']\n",
    "    classification_correct = sum(1 for r in classification_results if r['true_class'] == r['classification_pred'])\n",
    "    classification_accuracy = classification_correct / len(classification_results) if classification_results else 0\n",
    "    \n",
    "    # Afficher les métriques\n",
    "    print(f\"Résultats sur {len(results)} images de test:\")\n",
    "    print(f\"Précision de détection: {detection_accuracy:.2f} ({detection_correct}/{len(results)})\")\n",
    "    print(f\"Précision de classification: {classification_accuracy:.2f} ({classification_correct}/{len(classification_results)})\")\n",
    "    \n",
    "    # Créer une matrice de confusion pour la détection\n",
    "    detection_true = [r['true_detection'] for r in results]\n",
    "    detection_pred = [r['detection_pred'] for r in results]\n",
    "    detection_classes_unique = sorted(list(set(detection_true + detection_pred)))\n",
    "    \n",
    "    detection_cm = confusion_matrix(detection_true, detection_pred, labels=detection_classes_unique)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    sns.heatmap(detection_cm, annot=True, fmt='d', cmap='Blues', xticklabels=detection_classes_unique, yticklabels=detection_classes_unique)\n",
    "    plt.title('Matrice de confusion - Détection')\n",
    "    plt.ylabel('Vraie classe')\n",
    "    plt.xlabel('Classe prédite')\n",
    "    \n",
    "    # Créer une matrice de confusion pour la classification\n",
    "    if classification_results:\n",
    "        classification_true = [r['true_class'] for r in classification_results]\n",
    "        classification_pred = [r['classification_pred'] for r in classification_results]\n",
    "        classification_classes_unique = sorted(list(set([c for c in classification_true + classification_pred if c is not None])))\n",
    "        \n",
    "        classification_cm = confusion_matrix(classification_true, classification_pred, labels=classification_classes_unique)\n",
    "        \n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.heatmap(classification_cm, annot=True, fmt='d', cmap='Blues', xticklabels=classification_classes_unique, yticklabels=classification_classes_unique)\n",
    "        plt.title('Matrice de confusion - Classification')\n",
    "        plt.ylabel('Vraie classe')\n",
    "        plt.xlabel('Classe prédite')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Sauvegarder la figure dans Google Drive si nous sommes dans Colab\n",
    "    if IN_COLAB:\n",
    "        # Sauvegarder la figure localement\n",
    "        plt.savefig('confusion_matrices.png')\n",
    "        # Copier la figure dans Google Drive\n",
    "        confusion_matrices_path = f\"{DRIVE_PROJECT_DIR}/confusion_matrices.png\"\n",
    "        shutil.copy('confusion_matrices.png', confusion_matrices_path)\n",
    "        print(f\"Matrices de confusion sauvegardées dans Google Drive: {confusion_matrices_path}\")\n",
    "    \n",
    "    # Afficher quelques exemples de prédictions\n",
    "    num_examples = min(5, len(results))\n",
    "    plt.figure(figsize=(15, 4 * num_examples))\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        result = results[i]\n",
    "        img = Image.open(result['img_path']).convert('RGB')\n",
    "        \n",
    "        plt.subplot(num_examples, 2, 2*i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Image: {result['img_name']}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(num_examples, 2, 2*i + 2)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Texte de prédiction\n",
    "        text = f\"Vraie détection: {result['true_detection']}\\n\"\n",
    "        text += f\"Prédiction de détection: {result['detection_pred']} ({result['probs']['detection_prob']:.2f})\\n\\n\"\n",
    "        \n",
    "        if result['true_class'] is not None:\n",
    "            text += f\"Vraie classe: {result['true_class']}\\n\"\n",
    "        else:\n",
    "            text += \"Vraie classe: N/A\\n\"\n",
    "        \n",
    "        if result['classification_pred'] is not None:\n",
    "            text += f\"Prédiction de classification: {result['classification_pred']} ({result['probs']['classification_prob']:.2f})\"\n",
    "        else:\n",
    "            text += \"Prédiction de classification: N/A\"\n",
    "        \n",
    "        plt.text(0.1, 0.5, text, fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Sauvegarder la figure dans Google Drive si nous sommes dans Colab\n",
    "    if IN_COLAB:\n",
    "        # Sauvegarder la figure localement\n",
    "        plt.savefig('prediction_examples.png')\n",
    "        # Copier la figure dans Google Drive\n",
    "        prediction_examples_path = f\"{DRIVE_PROJECT_DIR}/prediction_examples.png\"\n",
    "        shutil.copy('prediction_examples.png', prediction_examples_path)\n",
    "        print(f\"Exemples de prédictions sauvegardés dans Google Drive: {prediction_examples_path}\")\n",
    "    \n",
    "    # Sauvegarder les résultats dans Google Drive si nous sommes dans Colab\n",
    "    if IN_COLAB:\n",
    "        # Convertir les résultats en format JSON sérialisable\n",
    "        serializable_results = []\n",
    "        for r in results:\n",
    "            serializable_result = {\n",
    "                'img_name': r['img_name'],\n",
    "                'true_detection': r['true_detection'],\n",
    "                'true_class': r['true_class'],\n",
    "                'detection_pred': r['detection_pred'],\n",
    "                'classification_pred': r['classification_pred'],\n",
    "                'detection_prob': r['probs']['detection_prob']\n",
    "            }\n",
    "            if 'classification_prob' in r['probs']:\n",
    "                serializable_result['classification_prob'] = r['probs']['classification_prob']\n",
    "            serializable_results.append(serializable_result)\n",
    "        \n",
    "        # Sauvegarder les résultats\n",
    "        test_results_path = f\"{DRIVE_PROJECT_DIR}/test_results.json\"\n",
    "        with open(test_results_path, 'w') as f:\n",
    "            json.dump(serializable_results, f, indent=4)\n",
    "        \n",
    "        print(f\"Résultats des tests sauvegardés dans Google Drive: {test_results_path}\")\n",
    "\n",
    "# Tester le modèle sur toutes les images de test\n",
    "if 'model' in locals():\n",
    "    print(\"Test du modèle sur toutes les images de test...\")\n",
    "    results = predict_all_test_images(model, TEST_IMAGES_DIR, detection_classes, classification_classes)\n",
    "    \n",
    "    if results:\n",
    "        visualize_test_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test interactif avec téléchargement d'images (pour Google Colab)\n",
    "\n",
    "Si vous êtes dans Google Colab, vous pouvez télécharger vos propres images pour les tester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB and 'model' in locals():\n",
    "    from google.colab import files\n",
    "    \n",
    "    def test_uploaded_image():\n",
    "        print(\"Veuillez télécharger une image à tester.\")\n",
    "        uploaded = files.upload()\n",
    "        \n",
    "        for filename in uploaded.keys():\n",
    "            # Sauvegarder l'image téléchargée\n",
    "            img_path = os.path.join(TEST_IMAGES_DIR, filename)\n",
    "            with open(img_path, 'wb') as f:\n",
    "                f.write(uploaded[filename])\n",
    "            \n",
    "            # Sauvegarder également dans Google Drive\n",
    "            drive_img_path = os.path.join(DRIVE_TEST_IMAGES_DIR, filename)\n",
    "            shutil.copy(img_path, drive_img_path)\n",
    "            \n",
    "            print(f\"Test du modèle sur l'image téléchargée: {filename}\")\n",
    "            detection_pred, classification_pred, probs = predict_single_image(model, img_path, detection_classes, classification_classes)\n",
    "            \n",
    "            if detection_pred is not None:\n",
    "                print(f\"Prédiction de détection: {detection_pred} (probabilité: {probs['detection_prob']:.2f})\")\n",
    "                if classification_pred is not None:\n",
    "                    print(f\"Prédiction de classification: {classification_pred} (probabilité: {probs['classification_prob']:.2f})\")\n",
    "                else:\n",
    "                    print(\"Prédiction de classification: N/A\")\n",
    "                \n",
    "                # Visualiser la prédiction\n",
    "                visualize_prediction(img_path, detection_pred, classification_pred, probs)\n",
    "            else:\n",
    "                print(\"Erreur lors de la prédiction.\")\n",
    "    \n",
    "    # Décommentez la ligne suivante pour tester avec vos propres images\n",
    "    # test_uploaded_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "Dans ce notebook, nous avons utilisé les fonctions existantes des modules `data_preprocessing` et `model_architecture` pour :\n",
    "1. Charger les modèles entraînés dans les notebooks précédents à partir de Google Drive\n",
    "2. Télécharger et préparer des images de test à partir du jeu de données Kaggle \"African Plums Dataset\"\n",
    "3. Tester le modèle sur des images individuelles\n",
    "4. Évaluer les performances du modèle sur un ensemble d'images de test\n",
    "5. Tester le modèle sur des images téléchargées par l'utilisateur (dans Google Colab)\n",
    "6. Sauvegarder les résultats et les figures dans Google Drive pour une utilisation ultérieure\n",
    "\n",
    "Le modèle est maintenant prêt à être utilisé pour classifier des prunes africaines en fonction de leur qualité et de leurs défauts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
