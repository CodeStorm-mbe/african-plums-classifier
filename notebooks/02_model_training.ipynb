{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînement du modèle de classification des prunes africaines\n",
    "\n",
    "Ce notebook présente le processus d'entraînement du modèle de classification des prunes africaines. Nous utiliserons une approche en deux étapes :\n",
    "1. **Détection** : Déterminer si l'image contient une prune ou non\n",
    "2. **Classification** : Si une prune est détectée, classifier son état\n",
    "\n",
    "## Table des matières\n",
    "\n",
    "1. [Configuration de l'environnement](#1-configuration-de-lenvironnement)\n",
    "2. [Chargement des données](#2-chargement-des-données)\n",
    "3. [Architecture du modèle](#3-architecture-du-modèle)\n",
    "4. [Entraînement du modèle de détection](#4-entraînement-du-modèle-de-détection)\n",
    "5. [Entraînement du modèle de classification](#5-entraînement-du-modèle-de-classification)\n",
    "6. [Évaluation des performances](#6-évaluation-des-performances)\n",
    "7. [Sauvegarde du modèle](#7-sauvegarde-du-modèle)\n",
    "8. [Conclusion](#8-conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration de l'environnement\n",
    "\n",
    "Commençons par importer les bibliothèques nécessaires et configurer notre environnement de travail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Ajouter le répertoire parent au chemin pour pouvoir importer nos modules\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Importer nos modules personnalisés\n",
    "from data.data_preprocessing import (\n",
    "    load_and_prepare_data,\n",
    "    load_and_prepare_two_stage_data,\n",
    "    visualize_batch\n",
    ")\n",
    "from models.model_architecture import (\n",
    "    get_model,\n",
    "    get_two_stage_model,\n",
    "    PlumClassifier,\n",
    "    LightweightPlumClassifier,\n",
    "    TwoStageModel\n",
    ")\n",
    "\n",
    "# Définir les chemins des données (à modifier selon votre configuration)\n",
    "DATA_ROOT = \"../data/raw\"  # Chemin vers le répertoire de données brutes\n",
    "PLUM_DATA_DIR = os.path.join(DATA_ROOT, \"plums\")  # Sous-dossier pour les prunes\n",
    "NON_PLUM_DATA_DIR = os.path.join(DATA_ROOT, \"non_plums\")  # Sous-dossier pour les non-prunes\n",
    "MODELS_DIR = \"../models/saved\"  # Répertoire pour sauvegarder les modèles entraînés\n",
    "\n",
    "# Créer les répertoires s'ils n'existent pas\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(PLUM_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(NON_PLUM_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Définir les paramètres d'entraînement\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 224\n",
    "NUM_WORKERS = 4\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 25\n",
    "EARLY_STOPPING_PATIENCE = 7\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Fixer les seeds pour la reproductibilité\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Déterminer le device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilisation de: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des données\n",
    "\n",
    "Chargeons les données préparées pour l'entraînement et la validation de nos modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier si les répertoires de données existent et contiennent des images\n",
    "def check_data_availability():\n",
    "    \"\"\"Vérifie si les données sont disponibles pour l'entraînement.\"\"\"\n",
    "    # Vérifier le répertoire des prunes\n",
    "    plum_classes = [d for d in os.listdir(PLUM_DATA_DIR) if os.path.isdir(os.path.join(PLUM_DATA_DIR, d))]\n",
    "    if not plum_classes:\n",
    "        print(f\"Aucune classe de prune trouvée dans {PLUM_DATA_DIR}. Veuillez ajouter des données ou créer des exemples.\")\n",
    "        return False\n",
    "    \n",
    "    # Vérifier s'il y a des images dans chaque classe\n",
    "    for cls in plum_classes:\n",
    "        cls_dir = os.path.join(PLUM_DATA_DIR, cls)\n",
    "        images = [f for f in os.listdir(cls_dir) \n",
    "                 if os.path.isfile(os.path.join(cls_dir, f)) and \n",
    "                 f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
    "        if not images:\n",
    "            print(f\"Aucune image trouvée dans la classe {cls}. Veuillez ajouter des données.\")\n",
    "            return False\n",
    "    \n",
    "    # Vérifier le répertoire des non-prunes\n",
    "    non_plum_dir = os.path.join(NON_PLUM_DATA_DIR, \"non_plum\")\n",
    "    if not os.path.exists(non_plum_dir):\n",
    "        print(f\"Le répertoire {non_plum_dir} n'existe pas. Veuillez créer ce répertoire et y ajouter des images.\")\n",
    "        return False\n",
    "    \n",
    "    # Vérifier s'il y a des images dans le répertoire non-prune\n",
    "    non_plum_images = [f for f in os.listdir(non_plum_dir) \n",
    "                      if os.path.isfile(os.path.join(non_plum_dir, f)) and \n",
    "                      f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
    "    if not non_plum_images:\n",
    "        print(f\"Aucune image trouvée dans {non_plum_dir}. Veuillez ajouter des données.\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Vérifier la disponibilité des données\n",
    "data_available = check_data_availability()\n",
    "\n",
    "if data_available:\n",
    "    try:\n",
    "        # Charger et préparer les données pour les deux étapes\n",
    "        print(\"Chargement des données pour les deux étapes...\")\n",
    "        (detection_train_loader, detection_val_loader, detection_test_loader, detection_class_names), \\\n",
    "        (classification_train_loader, classification_val_loader, classification_test_loader, classification_class_names) = \\\n",
    "            load_and_prepare_two_stage_data(\n",
    "                PLUM_DATA_DIR, \n",
    "                NON_PLUM_DATA_DIR,\n",
    "                batch_size=BATCH_SIZE, \n",
    "                img_size=IMG_SIZE,\n",
    "                num_workers=NUM_WORKERS\n",
    "            )\n",
    "        \n",
    "        print(f\"Classes de détection: {detection_class_names}\")\n",
    "        print(f\"Classes de classification: {classification_class_names}\")\n",
    "        \n",
    "        # Afficher les tailles des datasets\n",
    "        print(f\"\\nTailles des datasets de détection:\")\n",
    "        print(f\"  - Entraînement: {len(detection_train_loader.dataset)} images\")\n",
    "        print(f\"  - Validation: {len(detection_val_loader.dataset)} images\")\n",
    "        print(f\"  - Test: {len(detection_test_loader.dataset)} images\")\n",
    "        \n",
    "        print(f\"\\nTailles des datasets de classification:\")\n",
    "        print(f\"  - Entraînement: {len(classification_train_loader.dataset)} images\")\n",
    "        print(f\"  - Validation: {len(classification_val_loader.dataset)} images\")\n",
    "        print(f\"  - Test: {len(classification_test_loader.dataset)} images\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement des données: {e}\")\n",
    "else:\n",
    "    print(\"Veuillez d'abord créer des données d'exemple ou ajouter vos propres images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Architecture du modèle\n",
    "\n",
    "Examinons l'architecture des modèles que nous allons utiliser pour la détection et la classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer et afficher le modèle de détection\n",
    "def create_detection_model():\n",
    "    \"\"\"Crée le modèle de détection (prune vs non-prune).\"\"\"\n",
    "    model = get_model(\n",
    "        model_name='lightweight', \n",
    "        num_classes=2,  # 2 classes: prune et non-prune\n",
    "        base_model='mobilenet_v2', \n",
    "        pretrained=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Créer et afficher le modèle de classification\n",
    "def create_classification_model(num_classes):\n",
    "    \"\"\"Crée le modèle de classification des prunes.\"\"\"\n",
    "    model = get_model(\n",
    "        model_name='standard', \n",
    "        num_classes=num_classes,\n",
    "        base_model='resnet18', \n",
    "        pretrained=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Afficher le résumé des modèles\n",
    "if data_available and 'classification_class_names' in locals():\n",
    "    # Créer les modèles\n",
    "    detection_model = create_detection_model()\n",
    "    classification_model = create_classification_model(len(classification_class_names))\n",
    "    \n",
    "    # Afficher les informations sur les modèles\n",
    "    print(\"=== Modèle de détection ===\")\n",
    "    print(f\"Type: {detection_model.__class__.__name__}\")\n",
    "    print(f\"Informations: {detection_model.get_model_info()}\")\n",
    "    \n",
    "    print(\"\\n=== Modèle de classification ===\")\n",
    "    print(f\"Type: {classification_model.__class__.__name__}\")\n",
    "    print(f\"Informations: {classification_model.get_model_info()}\")\n",
    "    \n",
    "    # Afficher le nombre de paramètres\n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nNombre de paramètres entraînables:\")\n",
    "    print(f\"  - Modèle de détection: {count_parameters(detection_model):,}\")\n",
    "    print(f\"  - Modèle de classification: {count_parameters(classification_model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entraînement du modèle de détection\n",
    "\n",
    "Entraînons d'abord le modèle de détection qui déterminera si une image contient une prune ou non."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "                device, num_epochs=25, early_stopping_patience=7, save_dir=MODELS_DIR, model_name=\"model\"):\n",
    "    \"\"\"Entraîne le modèle et sauvegarde le meilleur modèle.\"\"\"\n",
    "    # Créer le répertoire de sauvegarde s'il n'existe pas\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialiser les variables\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = 0.0\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    # Historique pour tracer les courbes\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    # Heure de début\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Boucle d'entraînement\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Époque {epoch+1}/{num_epochs}\")\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Mode entraînement\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        # Boucle sur les batches d'entraînement\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Réinitialiser les gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Backward pass et optimisation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Statistiques\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        # Calculer les métriques d'entraînement\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_train_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        \n",
    "        # Mode évaluation\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        # Boucle sur les batches de validation\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistiques\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        # Calculer les métriques de validation\n",
    "        epoch_val_loss = running_loss / len(val_loader.dataset)\n",
    "        epoch_val_acc = running_corrects.double() / len(val_loader.dataset)\n",
    "        \n",
    "        # Ajuster le learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step(epoch_val_loss)\n",
    "        \n",
    "        # Afficher les métriques\n",
    "        print(f\"Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "        \n",
    "        # Mettre à jour l'historique\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['train_acc'].append(epoch_train_acc.item())\n",
    "        history['val_acc'].append(epoch_val_acc.item())\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        # Sauvegarder le meilleur modèle selon la perte de validation\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            print(f\"Amélioration de la perte de validation de {best_val_loss:.4f} à {epoch_val_loss:.4f}. Sauvegarde du modèle...\")\n",
    "            best_val_loss = epoch_val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, f'{model_name}_best_loss.pth'))\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "        \n",
    "        # Sauvegarder le meilleur modèle selon l'accuracy de validation\n",
    "        if epoch_val_acc > best_val_acc:\n",
    "            print(f\"Amélioration de l'accuracy de validation de {best_val_acc:.4f} à {epoch_val_acc:.4f}. Sauvegarde du modèle...\")\n",
    "            best_val_acc = epoch_val_acc\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, f'{model_name}_best_acc.pth'))\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            print(f\"Early stopping après {early_stopping_counter} époques sans amélioration\")\n",
    "            break\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Temps total d'entraînement\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print(f\"Entraînement terminé en {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
    "    print(f\"Meilleure perte de validation: {best_val_loss:.4f}\")\n",
    "    print(f\"Meilleure accuracy de validation: {best_val_acc:.4f}\")\n",
    "    \n",
    "    # Sauvegarder le dernier modèle\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f'{model_name}_last.pth'))\n",
    "    \n",
    "    # Sauvegarder l'historique\n",
    "    with open(os.path.join(save_dir, f'{model_name}_history.json'), 'w') as f:\n",
    "        json.dump(history, f)\n",
    "    \n",
    "    return history\n",
    "\n",
    "def plot_training_history(history, save_dir=MODELS_DIR, model_name=\"model\"):\n",
    "    \"\"\"Trace les courbes d'entraînement.\"\"\"\n",
    "    # Créer le répertoire de sauvegarde s'il n'existe pas\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Tracer les courbes de perte\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Graphique des pertes\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Époque')\n",
    "    plt.ylabel('Perte')\n",
    "    plt.legend()\n",
    "    plt.title('Évolution des pertes')\n",
    "    \n",
    "    # Graphique de l'accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.xlabel('Époque')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Évolution de l\\'accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f'{model_name}_training_curves.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Tracer l'évolution du learning rate\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history['lr'])\n",
    "    plt.xlabel('Époque')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Évolution du Learning Rate')\n",
    "    plt.yscale('log')\n",
    "    plt.savefig(os.path.join(save_dir, f'{model_name}_learning_rate.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le modèle de détection\n",
    "if data_available and 'detection_train_loader' in locals() and 'detection_val_loader' in locals():\n",
    "    try:\n",
    "        print(\"=== Entraînement du modèle de détection ===\\n\")\n",
    "        \n",
    "        # Créer le modèle de détection\n",
    "        detection_model = create_detection_model()\n",
    "        \n",
    "        # Déplacer le modèle sur le device\n",
    "        detection_model = detection_model.to(device)\n",
    "        \n",
    "        # Définir la fonction de perte et l'optimiseur\n",
    "        detection_criterion = nn.CrossEntropyLoss()\n",
    "        detection_optimizer = optim.Adam(detection_model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # Scheduler pour ajuster le learning rate\n",
    "        detection_scheduler = ReduceLROnPlateau(detection_optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "        \n",
    "        # Entraîner le modèle de détection\n",
    "        detection_history = train_model(\n",
    "            detection_model, \n",
    "            detection_train_loader, \n",
    "            detection_val_loader, \n",
    "            detection_criterion, \n",
    "            detection_optimizer, \n",
    "            detection_scheduler, \n",
    "            device, \n",
    "            num_epochs=NUM_EPOCHS, \n",
    "            early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "            save_dir=MODELS_DIR,\n",
    "            model_name=\"detection\"\n",
    "        )\n",
    "        \n",
    "        # Tracer les courbes d'entraînement\n",
    "        plot_training_history(detection_history, save_dir=MODELS_DIR, model_name=\"detection\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'entraînement du modèle de détection: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraînement du modèle de classification\n",
    "\n",
    "Maintenant, entraînons le modèle de classification qui déterminera l'état de la prune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le modèle de classification\n",
    "if data_available and 'classification_train_loader' in locals() and 'classification_val_loader' in locals():\n",
    "    try:\n",
    "        print(\"=== Entraînement du modèle de classification ===\\n\")\n",
    "        \n",
    "        # Créer le modèle de classification\n",
    "        classification_model = create_classification_model(len(classification_class_names))\n",
    "        \n",
    "        # Déplacer le modèle sur le device\n",
    "        classification_model = classification_model.to(device)\n",
    "        \n",
    "        # Définir la fonction de perte et l'optimiseur\n",
    "        classification_criterion = nn.CrossEntropyLoss()\n",
    "        classification_optimizer = optim.Adam(classification_model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # Scheduler pour ajuster le learning rate\n",
    "        classification_scheduler = ReduceLROnPlateau(classification_optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "        \n",
    "        # Entraîner le modèle de classification\n",
    "        classification_history = train_model(\n",
    "            classification_model, \n",
    "            classification_train_loader, \n",
    "            classification_val_loader, \n",
    "            classification_criterion, \n",
    "            classification_optimizer, \n",
    "            classification_scheduler, \n",
    "            device, \n",
    "            num_epochs=NUM_EPOCHS, \n",
    "            early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "            save_dir=MODELS_DIR,\n",
    "            model_name=\"classification\"\n",
    "        )\n",
    "        \n",
    "        # Tracer les courbes d'entraînement\n",
    "        plot_training_history(classification_history, save_dir=MODELS_DIR, model_name=\"classification\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'entraînement du modèle de classification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Évaluation des performances\n",
    "\n",
    "Évaluons les performances de nos modèles sur les ensembles de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device, class_names, save_dir=MODELS_DIR, model_name=\"model\"):\n",
    "    \"\"\"Évalue le modèle sur l'ensemble de test et génère des visualisations.\"\"\"\n",
    "    # Créer le répertoire de sauvegarde s'il n'existe pas\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Mode évaluation\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    # Pour la matrice de confusion\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Boucle sur les batches de test\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Statistiques\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        # Collecter les prédictions et les labels pour la matrice de confusion\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculer les métriques\n",
    "    test_loss = running_loss / len(test_loader.dataset)\n",
    "    test_acc = running_corrects.double() / len(test_loader.dataset)\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f} Acc: {test_acc:.4f}\")\n",
    "    \n",
    "    # Créer la matrice de confusion\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Visualiser la matrice de confusion\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Prédiction')\n",
    "    plt.ylabel('Vérité')\n",
    "    plt.title('Matrice de confusion')\n",
    "    plt.savefig(os.path.join(save_dir, f'{model_name}_confusion_matrix.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Générer le rapport de classification\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
    "    \n",
    "    # Sauvegarder le rapport\n",
    "    with open(os.path.join(save_dir, f'{model_name}_classification_report.json'), 'w') as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "    \n",
    "    # Visualiser les métriques par classe\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Extraire les métriques par classe\n",
    "    classes = list(report.keys())[:-3]  # Exclure 'accuracy', 'macro avg', 'weighted avg'\n",
    "    precision = [report[cls]['precision'] for cls in classes]\n",
    "    recall = [report[cls]['recall'] for cls in classes]\n",
    "    f1 = [report[cls]['f1-score'] for cls in classes]\n",
    "    \n",
    "    # Créer le graphique\n",
    "    x = np.arange(len(classes))\n",
    "    width = 0.25\n",
    "    \n",
    "    plt.bar(x - width, precision, width, label='Precision')\n",
    "    plt.bar(x, recall, width, label='Recall')\n",
    "    plt.bar(x + width, f1, width, label='F1-score')\n",
    "    \n",
    "    plt.xlabel('Classe')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Métriques par classe')\n",
    "    plt.xticks(x, classes, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f'{model_name}_metrics_by_class.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'test_loss': test_loss,\n",
    "        'test_acc': test_acc.item(),\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'classification_report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer le modèle de détection\n",
    "if data_available and 'detection_test_loader' in locals() and 'detection_class_names' in locals():\n",
    "    try:\n",
    "        print(\"=== Évaluation du modèle de détection ===\\n\")\n",
    "        \n",
    "        # Charger le meilleur modèle (selon l'accuracy)\n",
    "        detection_model = create_detection_model()\n",
    "        detection_model.load_state_dict(torch.load(os.path.join(MODELS_DIR, 'detection_best_acc.pth')))\n",
    "        detection_model = detection_model.to(device)\n",
    "        \n",
    "        # Évaluer le modèle\n",
    "        detection_criterion = nn.CrossEntropyLoss()\n",
    "        detection_metrics = evaluate_model(\n",
    "            detection_model, \n",
    "            detection_test_loader, \n",
    "            detection_criterion, \n",
    "            device, \n",
    "            detection_class_names,\n",
    "            save_dir=MODELS_DIR,\n",
    "            model_name=\"detection\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'évaluation du modèle de détection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer le modèle de classification\n",
    "if data_available and 'classification_test_loader' in locals() and 'classification_class_names' in locals():\n",
    "    try:\n",
    "        print(\"=== Évaluation du modèle de classification ===\\n\")\n",
    "        \n",
    "        # Charger le meilleur modèle (selon l'accuracy)\n",
    "        classification_model = create_classification_model(len(classification_class_names))\n",
    "        classification_model.load_state_dict(torch.load(os.path.join(MODELS_DIR, 'classification_best_acc.pth')))\n",
    "        classification_model = classification_model.to(device)\n",
    "        \n",
    "        # Évaluer le modèle\n",
    "        classification_criterion = nn.CrossEntropyLoss()\n",
    "        classification_metrics = evaluate_model(\n",
    "            classification_model, \n",
    "            classification_test_loader, \n",
    "            classification_criterion, \n",
    "            device, \n",
    "            classification_class_names,\n",
    "            save_dir=MODELS_DIR,\n",
    "            model_name=\"classification\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'évaluation du modèle de classification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sauvegarde du modèle\n",
    "\n",
    "Créons et sauvegardons le modèle à deux étapes complet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer et sauvegarder le modèle à deux étapes\n",
    "if data_available and 'detection_class_names' in locals() and 'classification_class_names' in locals():\n",
    "    try:\n",
    "        print(\"=== Création du modèle à deux étapes ===\\n\")\n",
    "        \n",
    "        # Charger les meilleurs modèles\n",
    "        detection_model = create_detection_model()\n",
    "        detection_model.load_state_dict(torch.load(os.path.join(MODELS_DIR, 'detection_best_acc.pth')))\n",
    "        \n",
    "        classification_model = create_classification_model(len(classification_class_names))\n",
    "        classification_model.load_state_dict(torch.load(os.path.join(MODELS_DIR, 'classification_best_acc.pth')))\n",
    "        \n",
    "        # Créer le modèle à deux étapes\n",
    "        two_stage_model = TwoStageModel(detection_model, classification_model, detection_threshold=0.7)\n",
    "        \n",
    "        # Sauvegarder les informations du modèle\n",
    "        model_info = {\n",
    "            'detection_classes': detection_class_names,\n",
    "            'classification_classes': classification_class_names,\n",
    "            'model_info': two_stage_model.get_model_info(),\n",
    "            'img_size': IMG_SIZE,\n",
    "            'date_created': time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(MODELS_DIR, 'two_stage_model_info.json'), 'w') as f:\n",
    "            json.dump(model_info, f, indent=4)\n",
    "        \n",
    "        print(\"Modèle à deux étapes créé et informations sauvegardées.\")\n",
    "        print(f\"Classes de détection: {detection_class_names}\")\n",
    "        print(f\"Classes de classification: {classification_class_names}\")\n",
    "        print(f\"Informations du modèle: {two_stage_model.get_model_info()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la création du modèle à deux étapes: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "Dans ce notebook, nous avons entraîné et évalué un modèle à deux étapes pour la classification des prunes africaines. Nous avons :\n",
    "\n",
    "1. Configuré notre environnement de travail\n",
    "2. Chargé les données préparées\n",
    "3. Créé et entraîné un modèle de détection (prune vs non-prune)\n",
    "4. Créé et entraîné un modèle de classification des prunes\n",
    "5. Évalué les performances des deux modèles\n",
    "6. Créé et sauvegardé un modèle à deux étapes complet\n",
    "\n",
    "Ces étapes nous ont permis de développer un système capable de détecter la présence de prunes dans une image et de classifier leur état. Dans le prochain notebook, nous explorerons comment utiliser ce modèle pour faire des prédictions sur de nouvelles images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prochaines étapes\n",
    "\n",
    "- Test du modèle sur de nouvelles images\n",
    "- Optimisation des hyperparamètres\n",
    "- Déploiement du modèle\n",
    "- Création d'une interface utilisateur"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
