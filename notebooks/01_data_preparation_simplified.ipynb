{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation des données pour le classificateur de prunes africaines\n",
    "\n",
    "Ce notebook utilise les fonctions existantes dans le dépôt pour préparer les données d'entraînement et de test du modèle de classification des prunes africaines en utilisant le jeu de données Kaggle \"African Plums Dataset\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration de l'environnement pour Google Colab\n",
    "\n",
    "Commençons par cloner le dépôt GitHub et configurer l'environnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier si nous sommes dans Google Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"Exécution dans Google Colab: {IN_COLAB}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Cloner le dépôt GitHub\n",
    "    !git clone https://github.com/CodeStorm-mbe/african-plums-classifier.git\n",
    "    %cd african-plums-classifier\n",
    "    \n",
    "    # Installer les dépendances requises\n",
    "    !pip install -r requirements.txt\n",
    "    !pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Monter Google Drive pour la persistance des données\n",
    "\n",
    "Pour conserver les données entre les différents notebooks, nous allons utiliser Google Drive comme stockage persistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monter Google Drive si nous sommes dans Colab\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Créer un répertoire pour notre projet dans Google Drive\n",
    "    DRIVE_PROJECT_DIR = \"/content/drive/MyDrive/african-plums-classifier\"\n",
    "    DRIVE_DATA_DIR = f\"{DRIVE_PROJECT_DIR}/data\"\n",
    "    DRIVE_KAGGLE_DIR = f\"{DRIVE_DATA_DIR}/kaggle\"\n",
    "    DRIVE_RAW_DATA_DIR = f\"{DRIVE_DATA_DIR}/raw\"\n",
    "    DRIVE_PLUM_DATA_DIR = f\"{DRIVE_RAW_DATA_DIR}/plums\"\n",
    "    DRIVE_NON_PLUM_DATA_DIR = f\"{DRIVE_RAW_DATA_DIR}/non_plums\"\n",
    "    \n",
    "    # Créer les répertoires s'ils n'existent pas\n",
    "    !mkdir -p {DRIVE_PROJECT_DIR}\n",
    "    !mkdir -p {DRIVE_DATA_DIR}\n",
    "    !mkdir -p {DRIVE_KAGGLE_DIR}\n",
    "    !mkdir -p {DRIVE_RAW_DATA_DIR}\n",
    "    !mkdir -p {DRIVE_PLUM_DATA_DIR}\n",
    "    !mkdir -p {DRIVE_NON_PLUM_DATA_DIR}\n",
    "    !mkdir -p {DRIVE_NON_PLUM_DATA_DIR}/non_plum\n",
    "    \n",
    "    print(f\"Google Drive monté et répertoires créés dans {DRIVE_PROJECT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Ajouter le répertoire courant au chemin pour pouvoir importer nos modules\n",
    "if IN_COLAB:\n",
    "    # Dans Colab, nous sommes déjà dans le répertoire du projet\n",
    "    if \"/content/african-plums-classifier\" not in sys.path:\n",
    "        sys.path.append(\"/content/african-plums-classifier\")\n",
    "else:\n",
    "    # En local, ajouter le répertoire parent\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "# Importer nos modules personnalisés\n",
    "from data.data_preprocessing import (\n",
    "    load_and_prepare_data,\n",
    "    load_and_prepare_two_stage_data,\n",
    "    visualize_batch,\n",
    "    analyze_dataset_distribution\n",
    ")\n",
    "\n",
    "# Définir les chemins des données\n",
    "if IN_COLAB:\n",
    "    # Utiliser les chemins dans Google Drive pour la persistance\n",
    "    DATA_ROOT = DRIVE_RAW_DATA_DIR\n",
    "    KAGGLE_DIR = DRIVE_KAGGLE_DIR\n",
    "    \n",
    "    # Créer également des liens symboliques pour faciliter l'accès depuis le code existant\n",
    "    LOCAL_DATA_ROOT = \"data/raw\"\n",
    "    LOCAL_KAGGLE_DIR = \"data/kaggle\"\n",
    "    \n",
    "    # Créer les répertoires locaux s'ils n'existent pas\n",
    "    !mkdir -p {LOCAL_DATA_ROOT}\n",
    "    !mkdir -p {LOCAL_KAGGLE_DIR}\n",
    "    \n",
    "    # Créer des liens symboliques si nécessaire\n",
    "    if not os.path.exists(LOCAL_DATA_ROOT) or not os.path.islink(LOCAL_DATA_ROOT):\n",
    "        !rm -rf {LOCAL_DATA_ROOT}\n",
    "        !ln -s {DATA_ROOT} {LOCAL_DATA_ROOT}\n",
    "    \n",
    "    if not os.path.exists(LOCAL_KAGGLE_DIR) or not os.path.islink(LOCAL_KAGGLE_DIR):\n",
    "        !rm -rf {LOCAL_KAGGLE_DIR}\n",
    "        !ln -s {KAGGLE_DIR} {LOCAL_KAGGLE_DIR}\n",
    "else:\n",
    "    # En local\n",
    "    DATA_ROOT = \"../data/raw\"\n",
    "    KAGGLE_DIR = \"../data/kaggle\"\n",
    "\n",
    "PLUM_DATA_DIR = os.path.join(DATA_ROOT, \"plums\")  # Sous-dossier pour les prunes\n",
    "NON_PLUM_DATA_DIR = os.path.join(DATA_ROOT, \"non_plums\")  # Sous-dossier pour les non-prunes\n",
    "\n",
    "# Vérifier si les répertoires existent\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(PLUM_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(NON_PLUM_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(NON_PLUM_DATA_DIR, \"non_plum\"), exist_ok=True)\n",
    "os.makedirs(KAGGLE_DIR, exist_ok=True)\n",
    "\n",
    "# Définir les paramètres\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 224\n",
    "NUM_WORKERS = 2 if IN_COLAB else 4  # Réduire le nombre de workers dans Colab\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Fixer les seeds pour la reproductibilité\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Déterminer le device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilisation de: {device}\")\n",
    "\n",
    "# Afficher les chemins des données\n",
    "print(f\"\\nChemins des données:\")\n",
    "print(f\"DATA_ROOT: {DATA_ROOT}\")\n",
    "print(f\"KAGGLE_DIR: {KAGGLE_DIR}\")\n",
    "print(f\"PLUM_DATA_DIR: {PLUM_DATA_DIR}\")\n",
    "print(f\"NON_PLUM_DATA_DIR: {NON_PLUM_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration de l'API Kaggle\n",
    "\n",
    "Pour télécharger le jeu de données Kaggle, nous devons configurer l'API Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de l'API Kaggle\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    \n",
    "    # Vérifier si le fichier kaggle.json existe déjà dans Google Drive\n",
    "    KAGGLE_CONFIG_PATH = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "    DRIVE_KAGGLE_CONFIG_PATH = f\"{DRIVE_PROJECT_DIR}/kaggle.json\"\n",
    "    \n",
    "    # Vérifier si le fichier kaggle.json existe dans Google Drive\n",
    "    kaggle_config_in_drive = os.path.exists(DRIVE_KAGGLE_CONFIG_PATH)\n",
    "    \n",
    "    # Vérifier si le fichier kaggle.json existe localement\n",
    "    kaggle_config_exists = os.path.exists(KAGGLE_CONFIG_PATH)\n",
    "    \n",
    "    if kaggle_config_in_drive:\n",
    "        print(f\"Fichier kaggle.json trouvé dans Google Drive. Utilisation de ce fichier.\")\n",
    "        # Créer le répertoire .kaggle s'il n'existe pas\n",
    "        os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n",
    "        # Copier le fichier de Google Drive vers le répertoire local\n",
    "        shutil.copy(DRIVE_KAGGLE_CONFIG_PATH, KAGGLE_CONFIG_PATH)\n",
    "        # Définir les permissions appropriées\n",
    "        os.chmod(KAGGLE_CONFIG_PATH, 600)\n",
    "        print(\"Fichier kaggle.json configuré avec succès.\")\n",
    "    elif not kaggle_config_exists:\n",
    "        print(\"Veuillez télécharger votre fichier kaggle.json pour l'authentification Kaggle.\")\n",
    "        print(\"Vous pouvez le générer sur https://www.kaggle.com/account dans la section 'API'.\")\n",
    "        \n",
    "        # Télécharger le fichier kaggle.json\n",
    "        uploaded = files.upload()\n",
    "        \n",
    "        # Créer le répertoire .kaggle s'il n'existe pas\n",
    "        os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n",
    "        \n",
    "        # Déplacer le fichier kaggle.json vers le répertoire .kaggle\n",
    "        if 'kaggle.json' in uploaded:\n",
    "            shutil.move('kaggle.json', KAGGLE_CONFIG_PATH)\n",
    "            # Définir les permissions appropriées\n",
    "            os.chmod(KAGGLE_CONFIG_PATH, 600)\n",
    "            # Sauvegarder également dans Google Drive pour une utilisation future\n",
    "            shutil.copy(KAGGLE_CONFIG_PATH, DRIVE_KAGGLE_CONFIG_PATH)\n",
    "            print(\"Fichier kaggle.json configuré avec succès et sauvegardé dans Google Drive.\")\n",
    "        else:\n",
    "            print(\"Erreur: Le fichier kaggle.json n'a pas été téléchargé.\")\n",
    "    else:\n",
    "        print(\"Le fichier kaggle.json existe déjà localement.\")\n",
    "        # Sauvegarder également dans Google Drive pour une utilisation future\n",
    "        shutil.copy(KAGGLE_CONFIG_PATH, DRIVE_KAGGLE_CONFIG_PATH)\n",
    "        print(\"Fichier kaggle.json sauvegardé dans Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Téléchargement et préparation du jeu de données Kaggle\n",
    "\n",
    "Téléchargeons le jeu de données \"African Plums Dataset\" de Kaggle et préparons-le pour notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_kaggle_dataset(force_download=False):\n",
    "    \"\"\"Télécharge le jeu de données Kaggle 'African Plums Dataset'.\"\"\"\n",
    "    # Vérifier si le jeu de données a déjà été téléchargé\n",
    "    dataset_zip = os.path.join(KAGGLE_DIR, 'african-plums-dataset.zip')\n",
    "    if os.path.exists(dataset_zip) and not force_download:\n",
    "        print(f\"Le jeu de données a déjà été téléchargé dans {dataset_zip}.\")\n",
    "        return dataset_zip\n",
    "    \n",
    "    print(\"Téléchargement du jeu de données Kaggle 'African Plums Dataset'...\")\n",
    "    try:\n",
    "        # Télécharger le jeu de données\n",
    "        !kaggle datasets download -d arnaudfadja/african-plums-quality-and-defect-assessment-data -p {KAGGLE_DIR}\n",
    "        print(f\"Jeu de données téléchargé avec succès dans {dataset_zip}.\")\n",
    "        return dataset_zip\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du téléchargement du jeu de données: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_and_organize_dataset(dataset_zip, force_extract=False):\n",
    "    \"\"\"Extrait et organise le jeu de données Kaggle pour notre modèle.\"\"\"\n",
    "    if not os.path.exists(dataset_zip):\n",
    "        print(f\"Le fichier {dataset_zip} n'existe pas.\")\n",
    "        return False\n",
    "    \n",
    "    # Vérifier si les données ont déjà été extraites\n",
    "    extracted_dir = os.path.join(KAGGLE_DIR, 'extracted')\n",
    "    if os.path.exists(extracted_dir) and not force_extract:\n",
    "        print(f\"Le jeu de données a déjà été extrait dans {extracted_dir}.\")\n",
    "    else:\n",
    "        print(f\"Extraction du jeu de données...\")\n",
    "        os.makedirs(extracted_dir, exist_ok=True)\n",
    "        \n",
    "        # Extraire le fichier zip\n",
    "        with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extracted_dir)\n",
    "        \n",
    "        print(f\"Jeu de données extrait avec succès dans {extracted_dir}.\")\n",
    "    \n",
    "    # Organiser les données pour notre modèle\n",
    "    print(\"Organisation des données pour notre modèle...\")\n",
    "    \n",
    "    # Vérifier la structure du jeu de données extrait\n",
    "    print(\"Structure du jeu de données extrait:\")\n",
    "    !find {extracted_dir} -type d | sort\n",
    "    \n",
    "    # Mapper les classes du jeu de données Kaggle aux classes de notre modèle\n",
    "    # Selon la description, les classes sont: bruised, cracked, rotten, spotted, unaffected, unripe\n",
    "    class_mapping = {\n",
    "        'bruised': 'bruised',\n",
    "        'cracked': 'cracked',\n",
    "        'rotten': 'rotten',\n",
    "        'spotted': 'spotted',\n",
    "        'unaffected': 'unaffected',\n",
    "        'unripe': 'unripe'\n",
    "    }\n",
    "    \n",
    "    # Créer les répertoires pour les classes de prunes\n",
    "    for cls in class_mapping.values():\n",
    "        os.makedirs(os.path.join(PLUM_DATA_DIR, cls), exist_ok=True)\n",
    "    \n",
    "    # Créer le répertoire pour les non-prunes\n",
    "    non_plum_dir = os.path.join(NON_PLUM_DATA_DIR, \"non_plum\")\n",
    "    os.makedirs(non_plum_dir, exist_ok=True)\n",
    "    \n",
    "    # Copier les images dans les répertoires appropriés\n",
    "    dataset_dir = os.path.join(extracted_dir, 'african_plums_dataset')\n",
    "    if os.path.exists(dataset_dir):\n",
    "        # Parcourir les sous-répertoires du jeu de données\n",
    "        for src_cls, dst_cls in class_mapping.items():\n",
    "            src_dir = os.path.join(dataset_dir, src_cls)\n",
    "            dst_dir = os.path.join(PLUM_DATA_DIR, dst_cls)\n",
    "            \n",
    "            if os.path.exists(src_dir):\n",
    "                # Copier les images\n",
    "                print(f\"Copie des images de {src_dir} vers {dst_dir}...\")\n",
    "                !cp -r {src_dir}/* {dst_dir}/\n",
    "            else:\n",
    "                print(f\"Le répertoire {src_dir} n'existe pas.\")\n",
    "        \n",
    "        print(\"Images copiées avec succès.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Le répertoire {dataset_dir} n'existe pas.\")\n",
    "        return False\n",
    "\n",
    "# Télécharger et préparer le jeu de données\n",
    "dataset_zip = download_kaggle_dataset(force_download=False)\n",
    "if dataset_zip:\n",
    "    success = extract_and_organize_dataset(dataset_zip, force_extract=False)\n",
    "    if success:\n",
    "        print(\"Jeu de données Kaggle préparé avec succès pour notre modèle.\")\n",
    "    else:\n",
    "        print(\"Erreur lors de la préparation du jeu de données Kaggle.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Création d'images non-prunes\n",
    "\n",
    "Le jeu de données Kaggle ne contient que des images de prunes. Pour notre modèle à deux étapes, nous avons également besoin d'images non-prunes. Nous allons télécharger quelques images non-prunes à partir d'un autre jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_non_plum_images(num_images=100):\n",
    "    \"\"\"Télécharge des images non-prunes à partir d'un autre jeu de données Kaggle.\"\"\"\n",
    "    non_plum_dir = os.path.join(NON_PLUM_DATA_DIR, \"non_plum\")\n",
    "    \n",
    "    # Vérifier si des images non-prunes existent déjà\n",
    "    existing_images = [f for f in os.listdir(non_plum_dir) if os.path.isfile(os.path.join(non_plum_dir, f))]\n",
    "    if existing_images:\n",
    "        print(f\"Des images non-prunes existent déjà ({len(existing_images)} images).\")\n",
    "        return\n",
    "    \n",
    "    print(\"Téléchargement d'images non-prunes...\")\n",
    "    \n",
    "    # Option 1: Télécharger des images de fruits (autres que des prunes) à partir d'un jeu de données Kaggle\n",
    "    try:\n",
    "        # Télécharger un jeu de données de fruits\n",
    "        !kaggle datasets download -d moltean/fruits -p {KAGGLE_DIR}\n",
    "        \n",
    "        # Extraire le jeu de données\n",
    "        fruits_zip = os.path.join(KAGGLE_DIR, 'fruits.zip')\n",
    "        fruits_dir = os.path.join(KAGGLE_DIR, 'fruits')\n",
    "        os.makedirs(fruits_dir, exist_ok=True)\n",
    "        \n",
    "        with zipfile.ZipFile(fruits_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(fruits_dir)\n",
    "        \n",
    "        # Sélectionner des images aléatoires (excluant les prunes)\n",
    "        import glob\n",
    "        all_fruit_images = []\n",
    "        for fruit_dir in glob.glob(os.path.join(fruits_dir, 'fruits-360/Training/*')):\n",
    "            fruit_name = os.path.basename(fruit_dir).lower()\n",
    "            if 'plum' not in fruit_name and 'prune' not in fruit_name:\n",
    "                all_fruit_images.extend(glob.glob(os.path.join(fruit_dir, '*.jpg')))\n",
    "        \n",
    "        # Sélectionner un sous-ensemble aléatoire\n",
    "        if all_fruit_images:\n",
    "            selected_images = random.sample(all_fruit_images, min(num_images, len(all_fruit_images)))\n",
    "            \n",
    "            # Copier les images sélectionnées\n",
    "            for i, img_path in enumerate(selected_images):\n",
    "                dst_path = os.path.join(non_plum_dir, f\"non_plum_{i+1}.jpg\")\n",
    "                shutil.copy(img_path, dst_path)\n",
    "            \n",
    "            print(f\"{len(selected_images)} images non-prunes copiées avec succès.\")\n",
    "        else:\n",
    "            print(\"Aucune image de fruit trouvée.\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du téléchargement d'images non-prunes: {e}\")\n",
    "        \n",
    "        # Option 2: Créer des images synthétiques si le téléchargement échoue\n",
    "        print(\"Création d'images non-prunes synthétiques...\")\n",
    "        \n",
    "        for i in range(num_images):\n",
    "            # Couleur aléatoire qui n'est pas proche des couleurs de prune\n",
    "            color = (random.randint(0, 100), random.randint(150, 255), random.randint(150, 255))\n",
    "            \n",
    "            # Créer une image\n",
    "            img = Image.new('RGB', (224, 224), (255, 255, 255))\n",
    "            pixels = img.load()\n",
    "            \n",
    "            # Dessiner une forme aléatoire (carré ou triangle)\n",
    "            shape = random.choice(['square', 'triangle'])\n",
    "            \n",
    "            if shape == 'square':\n",
    "                # Dessiner un carré\n",
    "                size = random.randint(100, 150)\n",
    "                top_left = (random.randint(0, 224-size), random.randint(0, 224-size))\n",
    "                \n",
    "                for x in range(top_left[0], top_left[0] + size):\n",
    "                    for y in range(top_left[1], top_left[1] + size):\n",
    "                        if 0 <= x < 224 and 0 <= y < 224:\n",
    "                            # Ajouter du bruit à chaque pixel\n",
    "                            pixel_color = [max(0, min(255, c + random.randint(-10, 10))) for c in color]\n",
    "                            pixels[x, y] = tuple(pixel_color)\n",
    "            else:\n",
    "                # Dessiner un triangle\n",
    "                p1 = (random.randint(50, 174), random.randint(50, 174))\n",
    "                p2 = (p1[0] + random.randint(30, 50), p1[1] + random.randint(30, 50))\n",
    "                p3 = (p1[0] - random.randint(0, 30), p2[1])\n",
    "                \n",
    "                # Remplir le triangle (algorithme simple)\n",
    "                min_x = min(p1[0], p2[0], p3[0])\n",
    "                max_x = max(p1[0], p2[0], p3[0])\n",
    "                min_y = min(p1[1], p2[1], p3[1])\n",
    "                max_y = max(p1[1], p2[1], p3[1])\n",
    "                \n",
    "                for x in range(min_x, max_x + 1):\n",
    "                    for y in range(min_y, max_y + 1):\n",
    "                        if 0 <= x < 224 and 0 <= y < 224:\n",
    "                            # Vérification simple si le point est dans le triangle\n",
    "                            if (x >= p1[0] and y >= p1[1] and x <= p2[0] and y <= p2[1]):\n",
    "                                # Ajouter du bruit à chaque pixel\n",
    "                                pixel_color = [max(0, min(255, c + random.randint(-10, 10))) for c in color]\n",
    "                                pixels[x, y] = tuple(pixel_color)\n",
    "            \n",
    "            # Sauvegarder l'image\n",
    "            img_path = os.path.join(non_plum_dir, f\"non_plum_{i+1}.jpg\")\n",
    "            img.save(img_path)\n",
    "        \n",
    "        print(f\"{num_images} images non-prunes synthétiques créées avec succès.\")\n",
    "        return True\n",
    "\n",
    "# Télécharger des images non-prunes\n",
    "download_non_plum_images(num_images=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploration des données\n",
    "\n",
    "Explorons les données que nous avons préparées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction simple pour explorer les répertoires de données\n",
    "def explore_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Le répertoire {directory} n'existe pas.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Contenu du répertoire {directory}:\")\n",
    "    subdirs = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "    print(f\"Sous-dossiers: {subdirs}\")\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        subdir_path = os.path.join(directory, subdir)\n",
    "        files = [f for f in os.listdir(subdir_path) if os.path.isfile(os.path.join(subdir_path, f))]\n",
    "        print(f\"  - {subdir}: {len(files)} fichiers\")\n",
    "\n",
    "# Explorer les répertoires de données\n",
    "print(\"=== Exploration des données de prunes ===\")\n",
    "explore_directory(PLUM_DATA_DIR)\n",
    "\n",
    "print(\"\\n=== Exploration des données non-prunes ===\")\n",
    "explore_directory(NON_PLUM_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyse de la distribution des classes\n",
    "\n",
    "Utilisons la fonction `analyze_dataset_distribution` du module `data_preprocessing` pour analyser la distribution des classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyser la distribution des classes\n",
    "try:\n",
    "    print(\"Analyse de la distribution des classes de prunes...\")\n",
    "    class_counts, distribution_img = analyze_dataset_distribution(PLUM_DATA_DIR)\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    print(\"\\nDistribution des classes de prunes:\")\n",
    "    for cls, count in class_counts.items():\n",
    "        print(f\"  - {cls}: {count} images\")\n",
    "    \n",
    "    # Afficher le graphique de distribution\n",
    "    if os.path.exists('class_distribution.png'):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        img = plt.imread('class_distribution.png')\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title('Distribution des classes')\n",
    "        plt.show()\n",
    "        \n",
    "        # Sauvegarder le graphique dans Google Drive si nous sommes dans Colab\n",
    "        if IN_COLAB:\n",
    "            distribution_img_path = f\"{DRIVE_PROJECT_DIR}/class_distribution.png\"\n",
    "            shutil.copy('class_distribution.png', distribution_img_path)\n",
    "            print(f\"Graphique de distribution sauvegardé dans Google Drive: {distribution_img_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de l'analyse de la distribution des classes: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualisation des exemples d'images\n",
    "\n",
    "Visualisons quelques exemples d'images de chaque classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_examples(data_dir, num_examples=3):\n",
    "    \"\"\"Visualise des exemples d'images de chaque classe.\"\"\"\n",
    "    import glob\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Obtenir les classes\n",
    "    classes = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    \n",
    "    # Créer une figure avec une ligne par classe\n",
    "    fig, axes = plt.subplots(len(classes), num_examples, figsize=(15, 3*len(classes)))\n",
    "    \n",
    "    # Pour chaque classe\n",
    "    for i, cls in enumerate(classes):\n",
    "        # Obtenir les chemins des images\n",
    "        img_paths = glob.glob(os.path.join(data_dir, cls, '*'))\n",
    "        \n",
    "        # Sélectionner des images aléatoires\n",
    "        selected_paths = random.sample(img_paths, min(num_examples, len(img_paths)))\n",
    "        \n",
    "        # Afficher chaque image\n",
    "        for j, path in enumerate(selected_paths):\n",
    "            img = Image.open(path)\n",
    "            if len(classes) > 1:\n",
    "                axes[i, j].imshow(img)\n",
    "                axes[i, j].set_title(f\"{cls} - {j+1}\")\n",
    "                axes[i, j].axis('off')\n",
    "            else:\n",
    "                axes[j].imshow(img)\n",
    "                axes[j].set_title(f\"{cls} - {j+1}\")\n",
    "                axes[j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualiser des exemples de prunes\n",
    "print(\"=== Exemples d'images de prunes ===\")\n",
    "visualize_examples(PLUM_DATA_DIR)\n",
    "\n",
    "# Visualiser des exemples de non-prunes\n",
    "print(\"\\n=== Exemples d'images non-prunes ===\")\n",
    "visualize_examples(NON_PLUM_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Préparation des données pour le modèle à deux étapes\n",
    "\n",
    "Utilisons la fonction `load_and_prepare_two_stage_data` du module `data_preprocessing` pour préparer les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier si les répertoires de données existent et contiennent des images\n",
    "def check_data_availability():\n",
    "    # Vérifier le répertoire des prunes\n",
    "    plum_classes = [d for d in os.listdir(PLUM_DATA_DIR) if os.path.isdir(os.path.join(PLUM_DATA_DIR, d))]\n",
    "    if not plum_classes:\n",
    "        print(f\"Aucune classe de prune trouvée dans {PLUM_DATA_DIR}. Veuillez ajouter des données.\")\n",
    "        return False\n",
    "    \n",
    "    # Vérifier le répertoire des non-prunes\n",
    "    non_plum_dir = os.path.join(NON_PLUM_DATA_DIR, \"non_plum\")\n",
    "    if not os.path.exists(non_plum_dir):\n",
    "        print(f\"Le répertoire {non_plum_dir} n'existe pas. Veuillez créer ce répertoire et y ajouter des images.\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Vérifier la disponibilité des données\n",
    "data_available = check_data_availability()\n",
    "\n",
    "if data_available:\n",
    "    try:\n",
    "        # Charger et préparer les données pour les deux étapes\n",
    "        print(\"Chargement des données pour les deux étapes...\")\n",
    "        (detection_train_loader, detection_val_loader, detection_test_loader, detection_class_names), \\\n",
    "        (classification_train_loader, classification_val_loader, classification_test_loader, classification_class_names) = \\\n",
    "            load_and_prepare_two_stage_data(\n",
    "                PLUM_DATA_DIR, \n",
    "                NON_PLUM_DATA_DIR,\n",
    "                batch_size=BATCH_SIZE, \n",
    "                img_size=IMG_SIZE,\n",
    "                num_workers=NUM_WORKERS\n",
    "            )\n",
    "        \n",
    "        print(f\"Classes de détection: {detection_class_names}\")\n",
    "        print(f\"Classes de classification: {classification_class_names}\")\n",
    "        \n",
    "        # Afficher les tailles des datasets\n",
    "        print(f\"\\nTailles des datasets de détection:\")\n",
    "        print(f\"  - Entraînement: {len(detection_train_loader.dataset)} images\")\n",
    "        print(f\"  - Validation: {len(detection_val_loader.dataset)} images\")\n",
    "        print(f\"  - Test: {len(detection_test_loader.dataset)} images\")\n",
    "        \n",
    "        print(f\"\\nTailles des datasets de classification:\")\n",
    "        print(f\"  - Entraînement: {len(classification_train_loader.dataset)} images\")\n",
    "        print(f\"  - Validation: {len(classification_val_loader.dataset)} images\")\n",
    "        print(f\"  - Test: {len(classification_test_loader.dataset)} images\")\n",
    "        \n",
    "        # Sauvegarder les informations des classes dans Google Drive si nous sommes dans Colab\n",
    "        if IN_COLAB:\n",
    "            import json\n",
    "            classes_info = {\n",
    "                'detection_class_names': detection_class_names,\n",
    "                'classification_class_names': classification_class_names\n",
    "            }\n",
    "            classes_info_path = f\"{DRIVE_PROJECT_DIR}/classes_info.json\"\n",
    "            with open(classes_info_path, 'w') as f:\n",
    "                json.dump(classes_info, f)\n",
    "            print(f\"Informations des classes sauvegardées dans Google Drive: {classes_info_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement des données: {e}\")\n",
    "else:\n",
    "    print(\"Veuillez d'abord ajouter des données dans les répertoires appropriés.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualisation d'un lot d'images\n",
    "\n",
    "Utilisons la fonction `visualize_batch` du module `data_preprocessing` pour visualiser un lot d'images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser un lot d'images\n",
    "if data_available and 'detection_train_loader' in locals():\n",
    "    try:\n",
    "        print(\"Visualisation d'un lot d'images de détection...\")\n",
    "        # Obtenir un lot d'images\n",
    "        images, labels = next(iter(detection_train_loader))\n",
    "        \n",
    "        # Visualiser le lot\n",
    "        visualize_batch(images, labels, detection_class_names)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la visualisation du lot d'images: {e}\")\n",
    "\n",
    "if data_available and 'classification_train_loader' in locals():\n",
    "    try:\n",
    "        print(\"\\nVisualisation d'un lot d'images de classification...\")\n",
    "        # Obtenir un lot d'images\n",
    "        images, labels = next(iter(classification_train_loader))\n",
    "        \n",
    "        # Visualiser le lot\n",
    "        visualize_batch(images, labels, classification_class_names)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la visualisation du lot d'images: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sauvegarde des informations de préparation des données\n",
    "\n",
    "Sauvegardons les informations importantes pour les notebooks suivants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder les informations importantes dans Google Drive\n",
    "if IN_COLAB and data_available:\n",
    "    import json\n",
    "    \n",
    "    # Créer un dictionnaire avec les informations importantes\n",
    "    data_prep_info = {\n",
    "        'data_root': DATA_ROOT,\n",
    "        'plum_data_dir': PLUM_DATA_DIR,\n",
    "        'non_plum_data_dir': NON_PLUM_DATA_DIR,\n",
    "        'img_size': IMG_SIZE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'detection_class_names': detection_class_names if 'detection_class_names' in locals() else None,\n",
    "        'classification_class_names': classification_class_names if 'classification_class_names' in locals() else None,\n",
    "        'data_prepared': True,\n",
    "        'date_prepared': import time; time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    \n",
    "    # Sauvegarder les informations dans Google Drive\n",
    "    data_prep_info_path = f\"{DRIVE_PROJECT_DIR}/data_prep_info.json\"\n",
    "    with open(data_prep_info_path, 'w') as f:\n",
    "        json.dump(data_prep_info, f, indent=4)\n",
    "    \n",
    "    print(f\"Informations de préparation des données sauvegardées dans Google Drive: {data_prep_info_path}\")\n",
    "    print(\"Ces informations seront utilisées par les notebooks suivants.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "Dans ce notebook, nous avons utilisé les fonctions existantes du module `data_preprocessing` pour :\n",
    "1. Télécharger et préparer le jeu de données Kaggle \"African Plums Dataset\"\n",
    "2. Télécharger des images non-prunes pour compléter notre jeu de données\n",
    "3. Explorer et analyser la distribution des classes\n",
    "4. Visualiser des exemples d'images\n",
    "5. Préparer les données pour notre modèle à deux étapes\n",
    "6. Sauvegarder les informations importantes dans Google Drive pour les notebooks suivants\n",
    "\n",
    "Les données sont maintenant prêtes pour l'entraînement du modèle dans le notebook suivant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
